Kevin	 Â C.	 Â Chang
Query Â Processing
Database	 Â Systems
1 	 Â 
	 Â 
â€¢ Logical/physical	 Â operators
â€¢Cost	 Â parameters	 Â and	 Â sorting	 Â 
â€¢One-Â­â€pass	 Â algorithms	 Â 
â€¢ Nested-Â­â€loop	 Â join	 Â 
â€¢Two-Â­â€pass	 Â algorithms
â€¢ Sort-Â­â€merge	 Â join
â€¢ Hash-Â­â€based	 Â join
â€¢ Index-Â­â€based	 Â algorithms
â€¢ Index-Â­â€based	 Â join
Concepts	 Â You	 Â Will	 Â Learn
Indexing	 Â (1	 Â of	 Â 66)Default	 Â Section	 Â (1	 Â of	 Â 2) 	 Â 
An	 Â outline	 Â of	 Â what	 Â will	 Â be	 Â covered	 Â 	 Â 
Logical/Physical	 Â Operators:	 Â more 
One-Â­â€pass	 Â algorithms:	 Â more 
Two-Â­â€pass	 Â algorithms:	 Â Similar	 Â to	 Â one-Â­â€
pass	 Â but	 Â makes	 Â two	 Â passes	 Â over	 Â the	 Â 
data:	 Â the	 Â first	 Â to	 Â assign	 Â temp	 Â labels	 Â and	 Â 
record	 Â equivalences	 Â and	 Â the	 Â second	 Â to	 Â 
the	 Â temp	 Â label. 
Index-Â­â€based	 Â algorithm:	 Â Didnâ€™t	 Â get	 Â to	 Â it. 
 
	 Â 
The	 Â Big	 Â Picture:	 Â Where	 Â We	 Â Are
Indexing Â (2 Â of Â 66)
Data	 Â Access
Data	 Â Modeling
Data/Query	 Â Processing
Data	 Â Acquisition
Relational NonRelational
St
ru
ct
ur
ed
Se
m
iS
tr
uc
tu
re
d
Transaction	 Â Management
N
oS
Q
L Â 
D
at
ab
as
es
XM
L Â 
D
at
ab
as
es
U
nt
ru
ct
ur
ed
Relational Â Databases
â€¢ SQL
â€¢ Relational Â Algebra
â€¢ Query Â Optimization
â€¢ Query Â Execution
â€¢ Indexing
â€¢ Concurrency Â Control
â€¢ Logging Â Recovery
Database Â Systems Toolkits
M
ap
 Â R
ed
uc
e
(P
ar
al
le
l)
St
or
m
 Â 
(S
tr
ea
m
)
Information Â Extraction
ER Â Ã ïƒ  Relational Â Model
Query	 Â Language
	 Â 
There	 Â are	 Â 2	 Â dimensions	 Â to	 Â data.	 Â 	 Â 
The	 Â first	 Â dimension	 Â -Â­â€	 Â In	 Â red,	 Â we	 Â can	 Â see	 Â 
the	 Â format	 Â of	 Â the	 Â data	 Â in	 Â the	 Â world.	 Â This	 Â 
comes	 Â as	 Â structured	 Â and	 Â unstructured.	 Â 
The	 Â second	 Â dimension	 Â can	 Â be	 Â derived	 Â 
from	 Â the	 Â question:	 Â Is	 Â it	 Â relational	 Â or	 Â not?	 Â 
(Content	 Â in	 Â blue).	 Â 	 Â 
The	 Â question	 Â thus	 Â far	 Â has	 Â been,	 Â given	 Â 
unstructured	 Â data,	 Â how	 Â do	 Â we	 Â get	 Â that	 Â 
into	 Â a	 Â structured	 Â database?	 Â 
As	 Â highlighted	 Â in	 Â the	 Â data,	 Â we	 Â can	 Â see	 Â the	 Â 
transition	 Â of	 Â the	 Â class	 Â from	 Â SQL	 Â to	 Â 
relational	 Â algebra,	 Â to	 Â XML	 Â and	 Â so	 Â forth.	 Â 
This	 Â is	 Â our	 Â exploration	 Â of	 Â Relational	 Â and	 Â 
Non-Â­â€relational.	 Â 	 Â 
Query Â Execution
4
Query compiler
Execution engine
Index/record mgr.
Buffer manager
Storage manager
storage
User/
Application
Query
or update
Query execution
plan
Record, index
requests
Page 
commands
Read/write
pages
	 Â 
The	 Â figure	 Â depicts	 Â a	 Â fine	 Â grain	 Â model	 Â of	 Â 
query	 Â execution.	 Â If	 Â we	 Â were	 Â to	 Â run	 Â a	 Â 
query,	 Â then	 Â we	 Â follow	 Â the	 Â path	 Â shown:	 Â 
An	 Â SQL	 Â command	 Â goes	 Â into	 Â a	 Â query	 Â 
complier.	 Â This	 Â will	 Â be	 Â processed	 Â by	 Â the	 Â 
execution	 Â engine,	 Â which	 Â runs	 Â on	 Â every	 Â 
operator.	 Â The	 Â index	 Â then	 Â comes	 Â in	 Â to	 Â give	 Â 
you	 Â the	 Â appropriate	 Â value	 Â (B-Â­â€tree,	 Â Hash	 Â 
etc	 Â are	 Â the	 Â index	 Â types	 Â we	 Â are	 Â referring	 Â 
to).	 Â Following	 Â that,	 Â they	 Â will	 Â come	 Â into	 Â 
the	 Â buffer	 Â manager.	 Â This	 Â is	 Â the	 Â virtual	 Â 
manager	 Â where	 Â it	 Â is	 Â decided	 Â what	 Â will	 Â 
come	 Â to	 Â the	 Â memory	 Â (This	 Â is	 Â the	 Â pin	 Â index	 Â 
to	 Â the	 Â memory).	 Â And	 Â finally,	 Â it	 Â goes	 Â to	 Â the	 Â 
storage	 Â manager	 Â that	 Â gets	 Â us	 Â the	 Â final	 Â 
output.	 Â 	 Â 	 Â 
â€¢ Logical	 Â operators
â€¢what they	 Â do
â€¢ e.g.,	 Â union,	 Â selection,	 Â project,	 Â join,	 Â grouping
â€¢Physical	 Â operators
â€¢ how they	 Â do	 Â it
â€¢ e.g.,	 Â nested	 Â loop	 Â join,	 Â sort-Â­â€merge	 Â join,	 Â hash	 Â join,	 Â index	 Â 
join
Logical	 Â v.s.	 Â Physical	 Â Operators
5 	 Â 
Query	 Â is	 Â simply	 Â a	 Â structure	 Â of	 Â â€œlegoâ€	 Â 
operators,	 Â thus	 Â all	 Â queries	 Â boil	 Â down	 Â to	 Â 
operators. 
Logical	 Â operators:	 Â has	 Â input	 Â and	 Â output. 
Physical	 Â operator:	 Â defined	 Â by	 Â method 
Logical/Physical	 Â Operators:	 Â more	 Â info. 
 
	 Â 
Query Â Execution Â Plans
6
Purchase Person
Buyer=name
City=â€˜urbanaâ€™ phone>â€™5430000â€™
buyer
(Simple Nested Loops)
SELECT S.sname
FROM Â  Â  Â  Â  Â Purchase Â P, Â Person Â Q
WHERE Â  Â P.buyer=Q.name Â AND
Q.city=â€˜urbanaâ€™ Â AND
Q.phone > Â â€˜5430000â€™ Â 
Ïƒ
Query Â Plan:
â€¢ logical Â tree
â€¢ implementation Â  Â  Â 
choice Â at Â every Â node
â€¢ scheduling Â of Â 
operations.
(Table scan) (Index scan)
Some Â operators Â are Â from Â relational
algebra, Â and Â others Â (e.g., Â scan, Â group) Â are Â not.
	 Â 
Given	 Â several	 Â building	 Â blocks,	 Â we	 Â get	 Â the	 Â 
alternative	 Â of	 Â having	 Â multiple	 Â ways	 Â of	 Â 
performing	 Â a	 Â task.	 Â This	 Â means	 Â that	 Â given	 Â 
a	 Â set	 Â of	 Â methods	 Â to	 Â perform	 Â a	 Â task,	 Â an	 Â 
optimal	 Â method	 Â may	 Â exist.	 Â 	 Â The	 Â diagram	 Â 
to	 Â the	 Â left	 Â shows	 Â a	 Â breakdown	 Â of	 Â the	 Â 
query	 Â listed.	 Â 	 Â 
Through	 Â query	 Â optimization,	 Â we	 Â can	 Â 
generate	 Â a	 Â tree	 Â as	 Â shown.	 Â 	 Â 
The	 Â iterator	 Â model.
â€¢Each	 Â operation	 Â is	 Â implemented	 Â by	 Â 3	 Â functions:
â€¢ Open:	 Â sets	 Â up	 Â the	 Â data	 Â structures	 Â and	 Â performs	 Â 
initializations
â€¢ GetNext:	 Â returns	 Â the	 Â the next	 Â tuple	 Â of	 Â the	 Â result.
â€¢ Close:	 Â ends	 Â the	 Â operations.	 Â Cleans	 Â up	 Â the	 Â data	 Â structures.
â€¢Enables	 Â pipelining!
How	 Â do	 Â We	 Â Combine	 Â Operations?
7 	 Â 
The	 Â iterator	 Â model	 Â is	 Â a	 Â model	 Â that	 Â we	 Â can	 Â 
use	 Â to	 Â help	 Â the	 Â query	 Â optimizing	 Â process.	 Â 
The	 Â iterator	 Â model	 Â means	 Â that	 Â you	 Â as	 Â a	 Â 
programmer	 Â will	 Â follow	 Â 3	 Â functions:	 Â 
Open,	 Â GetNext,	 Â and	 Â Close.	 Â 	 Â 
The	 Â iterator	 Â model	 Â also	 Â enables	 Â 
pipelining.	 Â This	 Â is	 Â simply	 Â the	 Â process	 Â line	 Â 
of	 Â different	 Â stages	 Â where	 Â every	 Â stage	 Â 
(such	 Â as	 Â selection,	 Â join,	 Â etc)	 Â is	 Â a	 Â busy	 Â 
process	 Â in	 Â some	 Â tuple.	 Â This	 Â means	 Â that	 Â if	 Â 
we	 Â have	 Â a	 Â multi-Â­â€core	 Â processor,	 Â then	 Â we	 Â 
are	 Â using	 Â all	 Â cores	 Â such	 Â that	 Â there	 Â is	 Â no	 Â 
waste	 Â in	 Â resources.	 Â 	 Â 
â€¢Cost	 Â parameters	 Â 	 Â 
â€¢ M	 Â =	 Â number	 Â of	 Â blocks	 Â that	 Â fit	 Â in	 Â main	 Â memory
â€¢ B(R)	 Â =	 Â number	 Â of	 Â blocks	 Â holding	 Â R
â€¢ T(R)	 Â =	 Â number	 Â of	 Â tuples	 Â in	 Â R
â€¢ V(R,a)	 Â =	 Â number	 Â of	 Â distinct	 Â values	 Â of	 Â the	 Â attribute	 Â a
â€¢Estimating	 Â the	 Â cost:
â€¢ Important	 Â in	 Â optimization	 Â (next	 Â lecture)
â€¢ Compute	 Â I/O	 Â cost	 Â only
â€¢ We	 Â compute	 Â the	 Â cost	 Â to	 Â read the	 Â tables	 Â 
â€¢ We	 Â donâ€™t	 Â compute	 Â the	 Â cost	 Â to	 Â write the	 Â result	 Â (because	 Â 
pipelining)
Cost	 Â Parameters
8 	 Â 
Sellinger:	 Â The	 Â director	 Â of	 Â the	 Â IBM	 Â 
computer	 Â science	 Â division.	 Â Worked	 Â on	 Â the	 Â 
system	 Â R	 Â project	 Â and	 Â worked	 Â on	 Â the	 Â 
concept	 Â of	 Â cost	 Â based	 Â optimization	 Â for	 Â 
queries.	 Â 	 Â 
Optimization	 Â fundamentally	 Â means	 Â 
searching	 Â for	 Â the	 Â best.	 Â 	 Â 
The	 Â concept	 Â 	 Â 
Since	 Â different	 Â query	 Â plans	 Â have	 Â different	 Â 
costs,	 Â Sellinger	 Â decided	 Â that	 Â this	 Â can	 Â be	 Â 
used	 Â to	 Â optimize	 Â the	 Â query.	 Â The	 Â 
parameters	 Â that	 Â are	 Â ideal	 Â for	 Â utilization	 Â 
for	 Â this	 Â are	 Â listed	 Â on	 Â the	 Â slide.	 Â 	 Â 
â€¢Two	 Â pass	 Â multi-Â­â€way	 Â merge	 Â sort
â€¢Step	 Â 1:
â€¢ Read	 Â M	 Â blocks	 Â at	 Â a	 Â time,	 Â sort,	 Â write
â€¢ Result:	 Â have	 Â runs	 Â of	 Â length	 Â M	 Â on	 Â disk
â€¢Step	 Â 2:
â€¢ Merge	 Â M-Â­â€1	 Â at	 Â a	 Â time,	 Â write	 Â to	 Â disk
â€¢ Result:	 Â have	 Â runs	 Â of	 Â length	 Â M(M-Â­â€1)â‰ˆM2
â€¢Cost:	 Â 3B(R),	 Â 	 Â Assumption:	 Â B(R)	 Â â‰¤M2
Sorting
9 	 Â 
**	 Â This	 Â is	 Â different	 Â from	 Â the	 Â sorting	 Â 
algorithms	 Â we	 Â have	 Â learned	 Â in	 Â our	 Â data	 Â 
structures	 Â courses.	 Â 	 Â 
For	 Â example:	 Â 	 Â 
Suppose	 Â we	 Â want	 Â to	 Â sort	 Â R	 Â such	 Â that,	 Â 
memory	 Â to	 Â disk	 Â =	 Â 1000,	 Â B(R)	 Â =	 Â 20kb.	 Â 
First	 Â we	 Â read	 Â R.	 Â Memory	 Â can	 Â only	 Â hold	 Â 
1000	 Â elements,	 Â so	 Â R	 Â will	 Â run	 Â 20	 Â times	 Â 
(each	 Â time	 Â 1000	 Â elements,	 Â 20k	 Â total).	 Â 
Merge	 Â after	 Â one	 Â at	 Â a	 Â time.	 Â  
We	 Â are	 Â assuming	 Â B(R)	 Â <=	 Â M2.	 Â If	 Â this	 Â is	 Â 
not	 Â satisfied,	 Â we	 Â will	 Â need	 Â more	 Â 
steps/iterations.	 Â That	 Â will	 Â make	 Â it	 Â more	 Â 
expensive.	 Â The	 Â cost	 Â is	 Â 3*B(R)	 Â if	 Â the	 Â 
assumption	 Â is	 Â qualified. 
.	 Â 	 Â 
â€¢The	 Â table	 Â is	 Â clustered	 Â (I.e.	 Â blocks	 Â consists	 Â only	 Â of	 Â 
records	 Â from	 Â this	 Â table):
â€¢ Table-Â­â€scan:	 Â if	 Â we	 Â know	 Â where	 Â the	 Â blocks	 Â are
â€¢ Index	 Â scan:	 Â if	 Â we	 Â have	 Â index	 Â to	 Â find	 Â the	 Â blocks
â€¢The	 Â table	 Â is	 Â unclustered (e.g.	 Â its	 Â records	 Â are	 Â placed	 Â 
on	 Â blocks	 Â with	 Â other	 Â tables)
â€¢ May	 Â need	 Â one	 Â read	 Â for	 Â each	 Â record
Scanning	 Â Tables
10 	 Â 
Tuples are clusters or unclustered.  
If	 Â table	 Â is	 Â clustered,	 Â we	 Â mean	 Â they	 Â are	 Â 
contiguous	 Â on	 Â disk.	 Â Thus,	 Â when	 Â reading,	 Â 
you	 Â spend	 Â B(R)	 Â time	 Â to	 Â read	 Â it. 
If	 Â tables	 Â are	 Â unclustered,	 Â it	 Â is	 Â possible	 Â 
that	 Â every	 Â record	 Â is	 Â placed	 Â in	 Â a	 Â different	 Â 
block.	 Â Worst	 Â case	 Â time	 Â spent	 Â reading	 Â it	 Â 
could	 Â be	 Â T(R). 
	 Â 	 Â 
	 Â 	 Â 
â€¢Clustered	 Â relation:
â€¢ Table	 Â scan:	 Â 	 Â B(R);	 Â to	 Â sort:	 Â 3B(R)
â€¢ Index	 Â scan:	 Â 	 Â B(R);	 Â to	 Â sort:	 Â B(R)	 Â or	 Â 3B(R)
â€¢Unclustered relation
â€¢ T(R);	 Â to	 Â sort:	 Â T(R)	 Â +	 Â 2B(R)
Cost	 Â of	 Â the	 Â Scan	 Â Operator
11 	 Â 
Professor	 Â didnâ€™t	 Â really	 Â touch	 Â on	 Â this	 Â oneâ€¦	 Â 
Selection	 Â Ïƒ(R),	 Â projection	 Â Î (R)
â€¢Both	 Â are	 Â tuple-Â­â€at-Â­â€a-Â­â€Time algorithms
â€¢Cost:	 Â B(R)
One-Â­â€pass	 Â Algorithms
12
Input buffer Output bufferUnary
operator
	 Â 
One-Â­â€pass	 Â algorithm	 Â 
A	 Â pass	 Â is	 Â a	 Â â€œphaseâ€	 Â of	 Â processing.	 Â Imagine	 Â 
this	 Â as	 Â one	 Â pass	 Â through	 Â the	 Â data.	 Â 	 Â 
At	 Â each	 Â pass,	 Â we	 Â have	 Â a	 Â unary	 Â operator,	 Â 
like	 Â a	 Â select	 Â or	 Â a	 Â projection	 Â that	 Â goes	 Â 
through	 Â the	 Â data	 Â and	 Â gives	 Â an	 Â output	 Â in	 Â 
the	 Â buffer.	 Â One	 Â tuple	 Â at	 Â a	 Â time	 Â is	 Â 
processed.	 Â 	 Â 
Duplicate	 Â elimination	 Â Î´(R)
â€¢Need	 Â to	 Â keep	 Â a	 Â dictionary	 Â in	 Â memory:
â€¢ balanced	 Â search	 Â tree
â€¢ hash	 Â table
â€¢ etc
â€¢Cost:	 Â B(R)
â€¢Assumption:	 Â B(Î´(R))	 Â <=	 Â M
One-Â­â€pass	 Â Algorithms
13 	 Â 
In	 Â the	 Â example	 Â of	 Â a	 Â duplicate	 Â elimination,	 Â 
we	 Â use	 Â a	 Â delta	 Â operator.	 Â 	 Â 
The	 Â operator	 Â DISTINCT	 Â is	 Â a	 Â great	 Â example	 Â 
of	 Â a	 Â way	 Â to	 Â eliminate	 Â duplicates.	 Â With	 Â the	 Â 
query	 Â SELECT	 Â DISTINCT	 Â names	 Â FROM	 Â 
users,	 Â we	 Â get	 Â all	 Â the	 Â distinct	 Â user	 Â names.	 Â 	 Â 
However,	 Â what	 Â this	 Â means	 Â is	 Â that	 Â we	 Â 
need	 Â to	 Â maintain	 Â a	 Â dictionary	 Â in	 Â memory.	 Â 	 Â 
If	 Â we	 Â have	 Â a	 Â tuple	 Â coming	 Â in,	 Â then	 Â what	 Â 
we	 Â do	 Â is	 Â check	 Â it	 Â in	 Â the	 Â dictionary.	 Â If	 Â the	 Â 
tuple	 Â does	 Â not	 Â exist,	 Â then	 Â it	 Â is	 Â outputted	 Â 
and	 Â added	 Â to	 Â the	 Â data	 Â structure.	 Â 
However,	 Â if	 Â it	 Â is,	 Â then	 Â it	 Â means	 Â that	 Â it	 Â is	 Â a	 Â 
duplicate.	 Â We	 Â are	 Â basically	 Â building	 Â a	 Â 
dictionary	 Â in	 Â the	 Â main	 Â memory.	 Â 	 Â 
Grouping:	 Â Î³city,	 Â sum(price) (R)
â€¢Need	 Â to	 Â keep	 Â a	 Â dictionary	 Â in	 Â memory
â€¢Also	 Â store	 Â the	 Â sum(price)	 Â for	 Â each	 Â city
â€¢Cost:	 Â B(R)
â€¢Assumption:	 Â number	 Â of	 Â cities	 Â fits	 Â in	 Â memory
One-Â­â€pass	 Â Algorithms
14 	 Â 
Furthermore,	 Â operators	 Â such	 Â as	 Â Grouping	 Â 
(union,	 Â etc)	 Â all	 Â follow	 Â this	 Â same	 Â use	 Â case.	 Â 	 Â 
Binary	 Â operations:	 Â R	 Â âˆ©	 Â S,	 Â R	 Â U	 Â S,	 Â R	 Â â€“ S
â€¢Assumption:	 Â min(B(R),	 Â B(S))	 Â <=	 Â M
â€¢Scan	 Â one	 Â table	 Â first,	 Â then	 Â the	 Â next,	 Â eliminate	 Â 
duplicates
â€¢Cost:	 Â B(R)+B(S)
One-Â­â€pass	 Â Algorithms
15 	 Â 
Self-Â­â€explanatory	 Â 
â€¢Tuple-Â­â€based	 Â nested	 Â loop	 Â ğ‘… Â  â‹ˆ  Â ğ‘†
â€¢R=outer	 Â relation,	 Â S=inner	 Â relation
for each	 Â tuple	 Â r	 Â in	 Â R	 Â do
for each	 Â tuple	 Â s	 Â in	 Â S	 Â do
if r	 Â and	 Â s	 Â join	 Â then output	 Â (r,s)
â€¢Cost:	 Â T(R)	 Â T(S),	 Â 	 Â sometimes	 Â T(R)	 Â B(S)
Nested	 Â Loop	 Â Joins
16 	 Â 
Join	 Â processes	 Â are	 Â very	 Â expensive.	 Â So	 Â 
we	 Â get	 Â the	 Â question	 Â of	 Â how	 Â we	 Â can	 Â 
optimize	 Â this.	 Â  
For	 Â first	 Â tuple	 Â of	 Â R,	 Â loop	 Â through	 Â S;	 Â for	 Â 
second	 Â tuple	 Â of	 Â R,	 Â loop	 Â through	 Â S	 Â again,	 Â 
etc.	 Â Thus,	 Â nested	 Â loop. 
	 Â 	 Â 
â€¢Block-Â­â€based	 Â Nested	 Â Loop	 Â Join
for each	 Â (M-Â­â€1)	 Â blocks	 Â bs of	 Â S	 Â do
for each	 Â block	 Â br of	 Â R	 Â do
for each	 Â tuple	 Â s	 Â in	 Â bs do
for each	 Â tuple	 Â r	 Â in	 Â br do
if r	 Â and	 Â s	 Â join	 Â then output(r,s)
Nested	 Â Loop	 Â Joins
17 	 Â 
To	 Â be	 Â smarter,	 Â store	 Â S	 Â in	 Â memory	 Â (can	 Â 
store	 Â M-Â­â€1	 Â items,	 Â 1	 Â reserved	 Â for	 Â R).	 Â Loop	 Â 
S	 Â through	 Â every	 Â tuple	 Â in	 Â R.	 Â Then	 Â store	 Â 
another	 Â M-Â­â€1	 Â items	 Â in	 Â S.	 Â Keep	 Â doing	 Â this	 Â 
until	 Â S	 Â is	 Â looped	 Â through.	 Â  
Total	 Â Cost:	 Â B(S)	 Â +	 Â B(R)	 Â X	 Â B(S)/M 
	 Â 
Nested Â Loop Â Joins
18
. . .
. . .
R & S
Blocks of S
(k < M-1 pages)
Input buffer for R Output buffer
. . .
Join Result
	 Â 
	 Â 
â€¢Block-Â­â€based	 Â Nested	 Â Loop	 Â Join
â€¢Cost:
â€¢ Read	 Â S	 Â once:	 Â cost	 Â B(S)
â€¢ Outer	 Â loop	 Â runs	 Â B(S)/(M-Â­â€1)	 Â times,	 Â and	 Â each	 Â time	 Â need	 Â to	 Â 
read	 Â R:	 Â costs	 Â B(S)B(R)/(M-Â­â€1)
â€¢ Total	 Â cost:	 Â 	 Â B(S)	 Â 	 Â +	 Â 	 Â B(S)B(R)/(M-Â­â€1)
â€¢Notice:	 Â it	 Â is	 Â better	 Â to	 Â iterate	 Â over	 Â the	 Â smaller	 Â 
relation	 Â firstâ€“ i.e.,	 Â S	 Â smaller
Nested	 Â Loop	 Â Joins
19 	 Â 
	 Â 
Two Â pass Â algorithms
20 	 Â 
Why	 Â do	 Â we	 Â need	 Â two	 Â pass	 Â algorithm	 Â to	 Â 
solve?	 Â -Â­â€-Â­â€	 Â Because	 Â itâ€™s	 Â not	 Â always	 Â possible	 Â 
to	 Â fit	 Â the	 Â relation	 Â data	 Â inside	 Â memory!	 Â 
Which	 Â means	 Â B(R)	 Â <=	 Â M	 Â does	 Â not	 Â hold.	 Â 
In	 Â the	 Â first	 Â pass:	 Â we	 Â organize	 Â data	 Â to	 Â get	 Â 
them	 Â prepared	 Â for	 Â our	 Â operation.	 Â E.g.	 Â 
sorting,	 Â hashing.	 Â 
In	 Â the	 Â second	 Â pass:	 Â we	 Â do	 Â the	 Â wanted	 Â 
operation	 Â based	 Â the	 Â prepared	 Â data.	 Â 
	 Â 
	 Â 
Duplicate	 Â elimination	 Â Î´(R)
â€¢Simple	 Â idea:	 Â like	 Â sorting,	 Â but	 Â include	 Â no	 Â duplicates
â€¢Step	 Â 1:	 Â sort	 Â runs	 Â of	 Â size	 Â M,	 Â write
â€¢ Cost:	 Â 2B(R)
â€¢Step	 Â 2:	 Â merge	 Â M-Â­â€1	 Â runs,	 Â 
but	 Â include	 Â each	 Â tuple	 Â only	 Â once
â€¢ Cost:	 Â B(R)
â€¢Total	 Â cost:	 Â 3B(R),	 Â Assumption:	 Â B(R) <=	 Â M2
Two-Â­â€Pass	 Â Algorithms	 Â Based	 Â on	 Â Sorting
21 	 Â 
Background	 Â information:	 Â Merge	 Â Sort.	 Â 	 Â 
1) Load	 Â M	 Â blocks	 Â a	 Â time	 Â into	 Â memory,	 Â 
do	 Â sort	 Â and	 Â output	 Â to	 Â the	 Â disk.	 Â 	 Â 
2) Merge	 Â by	 Â looking	 Â at	 Â the	 Â most	 Â outside	 Â 
element	 Â and	 Â merge	 Â by	 Â comparing	 Â 
them.	 Â 	 Â 
Here	 Â we	 Â assume	 Â B(R)<=	 Â M2	 Â because	 Â for	 Â 
the	 Â merging	 Â phase,	 Â we	 Â assume	 Â that	 Â the	 Â 
main	 Â memory	 Â can	 Â hold	 Â one	 Â element	 Â from	 Â 
each	 Â run,	 Â and	 Â thatâ€™s	 Â B(R)/M.	 Â So	 Â we	 Â have	 Â 
B(R)/M	 Â <=M	 Â and	 Â thus,	 Â B(R)<=	 Â M2.	 Â And	 Â 
remember	 Â that	 Â we	 Â shall	 Â have	 Â lotâ€™s	 Â of	 Â M2	 Â 
appearing	 Â next	 Â because	 Â we	 Â all	 Â assume	 Â the	 Â 
whole	 Â process	 Â has	 Â two	 Â pass,	 Â which	 Â means	 Â 
we	 Â donâ€™t	 Â need	 Â additional	 Â pass	 Â for	 Â 
merging.	 Â 
â€¢Selection?
â€¢Projection?
â€¢Set	 Â operations?
â€¢ Join?
â€¢Duplicate	 Â elimination?
â€¢Grouping?
Q:	 Â What	 Â can	 Â sorting	 Â help?	 Â And,	 Â how?
22 	 Â 
1) Selection	 Â -Â­â€	 Â Sorting	 Â will	 Â help	 Â selection	 Â 
but	 Â not	 Â quite,	 Â because	 Â it	 Â can	 Â help	 Â us	 Â 
find	 Â the	 Â range	 Â faster	 Â by	 Â making	 Â us	 Â 
just	 Â care	 Â about	 Â the	 Â boarder	 Â value.	 Â 
However,	 Â for	 Â equal	 Â selection	 Â it	 Â will	 Â 
not	 Â improve	 Â our	 Â finding	 Â a	 Â little	 Â bit	 Â by	 Â 
providing	 Â the	 Â chance	 Â of	 Â using	 Â binary	 Â 
search.	 Â 	 Â 
2) Projection	 Â -Â­â€	 Â Sorting	 Â will	 Â not	 Â help	 Â in	 Â 
projection	 Â because	 Â it	 Â cannot	 Â help	 Â us	 Â 
selecting	 Â on	 Â a	 Â specific	 Â attribute.	 Â 	 Â 
3) Set	 Â Operation	 Â -Â­â€	 Â Sorting	 Â can	 Â help	 Â in	 Â 
duplicate	 Â elimination,	 Â because	 Â with	 Â 
the	 Â duplicate	 Â attribute	 Â sorted,	 Â we	 Â 
can	 Â find	 Â out	 Â the	 Â duplicate	 Â values	 Â by	 Â 
comparing	 Â with	 Â neighboring	 Â values,	 Â 
and	 Â merge	 Â back	 Â only	 Â once	 Â for	 Â each	 Â 
value.	 Â 	 Â 
4) For	 Â other	 Â operations,	 Â see	 Â discussion	 Â 
below.	 Â 
Grouping:	 Â Î³city,	 Â sum(price) (R)
â€¢Same	 Â as	 Â before:	 Â sort,	 Â then	 Â compute	 Â the	 Â sum(price)	 Â 
for	 Â each	 Â group
â€¢As	 Â before:	 Â compute	 Â sum(price)	 Â during	 Â the	 Â merge	 Â 
phase.
â€¢Total	 Â cost:	 Â 3B(R)
â€¢Assumption:	 Â B(R) <=	 Â M2
Two-Â­â€Pass	 Â Algorithms	 Â Based	 Â on	 Â Sorting
23 	 Â 
Sorting	 Â will	 Â help	 Â for	 Â grouping	 Â because	 Â we	 Â 
can	 Â sort	 Â the	 Â group	 Â value	 Â and	 Â when	 Â 
merging,	 Â we	 Â can	 Â do	 Â the	 Â aggregation	 Â like	 Â 
sum(),	 Â average(),	 Â count()	 Â etc.	 Â 
Binary	 Â operations:	 Â R	 Â âˆ©	 Â S,	 Â R	 Â U	 Â S,	 Â R	 Â â€“ S
â€¢ Idea:	 Â sort	 Â R,	 Â sort	 Â S,	 Â then	 Â do	 Â the	 Â right	 Â thing
â€¢A	 Â closer	 Â look:
â€¢ Step	 Â 1:	 Â split	 Â R	 Â into	 Â runs	 Â of	 Â size	 Â M,	 Â then	 Â split	 Â S	 Â into	 Â runs	 Â of	 Â 
size	 Â M.	 Â 	 Â Cost:	 Â 2B(R)	 Â +	 Â 2B(S)
â€¢ Step	 Â 2:	 Â merge	 Â all	 Â x runs	 Â from	 Â R;	 Â merge	 Â all	 Â y runs	 Â from	 Â S;	 Â 
ouput	 Â a	 Â tuple	 Â on	 Â a	 Â case	 Â by	 Â cases	 Â basis	 Â (x +	 Â y <=	 Â M)
â€¢Total	 Â cost:	 Â 3B(R)+3B(S)
â€¢Assumption:	 Â B(R)+B(S)<=	 Â M2
Two-Â­â€Pass	 Â Algorithms	 Â Based	 Â on	 Â Sorting
24 	 Â 
Sorting	 Â can	 Â help	 Â binary	 Â operations	 Â 
because	 Â by	 Â sorting	 Â the	 Â two	 Â relations	 Â R	 Â 
and	 Â S,	 Â we	 Â can	 Â split	 Â the	 Â two	 Â relations	 Â and	 Â 
merge	 Â them	 Â back	 Â as	 Â tuples.	 Â 	 Â 
However,	 Â since	 Â when	 Â merging,	 Â we	 Â have	 Â 
to	 Â hold	 Â them	 Â all	 Â in	 Â the	 Â memory	 Â to	 Â form	 Â 
tuples,	 Â thus	 Â we	 Â shall	 Â have	 Â B(R)+B(S)<=M2.	 Â 
	 Â 
Join	 Â ğ‘… â‹ˆ ğ‘†
â€¢Start	 Â by	 Â sorting	 Â both	 Â R	 Â and	 Â S	 Â on	 Â the	 Â join	 Â attribute:
â€¢ Cost:	 Â 4B(R)+4B(S)	 Â 	 Â (because	 Â need	 Â to	 Â write	 Â to	 Â disk)
â€¢Read	 Â both	 Â relations	 Â in	 Â sorted	 Â order,	 Â match	 Â tuples
â€¢ Cost:	 Â B(R)+B(S)
â€¢Difficulty:	 Â many	 Â tuples	 Â in	 Â R	 Â may	 Â match	 Â many	 Â in	 Â S
â€¢ If	 Â at	 Â least	 Â one	 Â set	 Â of	 Â tuples	 Â fits	 Â in	 Â M,	 Â we	 Â are	 Â OK
â€¢ Otherwise	 Â need	 Â nested	 Â loop,	 Â higher	 Â cost
â€¢Total	 Â cost:	 Â 5B(R)+5B(S)
â€¢Assumption:	 Â B(R) <=	 Â M2,	 Â B(S) <=	 Â M2
Sort-Â­â€Merge	 Â Join
25 	 Â 
Sorting	 Â can	 Â also	 Â help	 Â join.	 Â This	 Â is	 Â done	 Â like	 Â 
this:	 Â We	 Â assume	 Â that	 Â we	 Â can	 Â have	 Â either	 Â 
tuples	 Â in	 Â R	 Â or	 Â S	 Â that	 Â can	 Â fit	 Â in	 Â M	 Â â€“	 Â 
min(B(R),	 Â B(S))<=M2.	 Â Then	 Â with	 Â S	 Â fit	 Â in	 Â 
memory,	 Â we	 Â can	 Â check	 Â one	 Â tuple	 Â in	 Â R	 Â 
with	 Â all	 Â the	 Â tuples	 Â in	 Â S	 Â each	 Â time	 Â to	 Â merge	 Â 
the	 Â result	 Â of	 Â join.	 Â 	 Â 
We	 Â can	 Â also	 Â do	 Â the	 Â join	 Â ahead	 Â by	 Â without	 Â 
writing	 Â the	 Â sorted	 Â R	 Â and	 Â S	 Â back	 Â to	 Â file.	 Â But	 Â 
this	 Â will	 Â require	 Â that	 Â we	 Â can	 Â hold	 Â both	 Â R	 Â 
and	 Â S	 Â in	 Â memory,	 Â with	 Â B(R)+B(S)<=M2	 Â 
The	 Â cost	 Â will	 Â be	 Â 3(B(R)+B(S))	 Â in	 Â this	 Â case	 Â 
while	 Â the	 Â size	 Â assumption	 Â is	 Â still	 Â 
B(R)+B(S)â‰¤ ğ‘€!.	 Â 
(B+-Â­â€tree	 Â and	 Â B+	 Â tree	 Â demo:	 Â 
B+-Â­â€Trees	 Â http://pfunproject.web.cs.illinois.edu/extra/FayezBPlusTree.html	 Â 
B+-Â­â€Trees	 Â http://www.seanster.com/BplusTree/BplusTree.html	 Â 
B-Â­â€Trees	 Â http://slady.cz/java/bt/	 Â 
B-Â­â€Trees	 Â http://idlebox.net/2007/stx-Â­â€btree/demo.htt	 Â 
Comparison:	 Â For	 Â B-Â­â€tree	 Â searching	 Â always	 Â 
needs	 Â to	 Â go	 Â to	 Â the	 Â leave	 Â nodes	 Â while	 Â for	 Â 
B+-Â­â€tree	 Â it	 Â may	 Â hit	 Â some	 Â nodes	 Â earlier)	 Â 
â€¢Pass	 Â 1?
â€¢Pass	 Â 2?
Q:	 Â Why	 Â is	 Â sorting-Â­â€based	 Â â€œtwoâ€	 Â pass?
26 	 Â 
For	 Â pass	 Â 1,	 Â we	 Â load	 Â M	 Â blocks	 Â into	 Â memory	 Â 
and	 Â do	 Â sorting	 Â before	 Â output	 Â to	 Â the	 Â disk.	 Â 
For	 Â pass	 Â 2,	 Â we	 Â merge	 Â them	 Â back	 Â and	 Â 
perform	 Â the	 Â intended	 Â operation	 Â in	 Â the	 Â 
meantime.	 Â 	 Â 
â€¢ Idea:	 Â partition	 Â a	 Â relation	 Â R	 Â into	 Â buckets,	 Â on	 Â disk
â€¢Each	 Â bucket	 Â has	 Â size	 Â approx.	 Â B(R)/M
â€¢Does	 Â each	 Â bucket	 Â fit	 Â in	 Â main	 Â memory	 Â ?
â€¢ Yes	 Â if	 Â B(R)/M	 Â <=	 Â M,	 Â 	 Â 	 Â i.e.	 Â B(R)	 Â <=	 Â M2
Two	 Â Pass	 Â Algorithms	 Â Based	 Â on	 Â Hashing
27
M main memory buffers DiskDisk
Relation R
OUTPUT
2INPUT
1
hash
function
h M-1
Partitions
1
2
M-1
. . .
1
2
B(R)
	 Â 
For	 Â the	 Â first	 Â pass:	 Â we	 Â do	 Â the	 Â hash	 Â 
operation	 Â and	 Â put	 Â the	 Â output	 Â into	 Â 
different	 Â buckets	 Â based	 Â on	 Â different	 Â hash	 Â 
result.	 Â 	 Â 
For	 Â the	 Â second	 Â pas:	 Â we	 Â do	 Â the	 Â intended	 Â 
operation	 Â on	 Â the	 Â partitioned	 Â blocks.	 Â 	 Â 
â€¢Selection?
â€¢Projection?
â€¢Set	 Â operations?
â€¢ Join?
â€¢Duplicate	 Â elimination?
â€¢Grouping?
Q:	 Â What	 Â can	 Â hashing	 Â help?	 Â And,	 Â how?
28 	 Â 
1) Hashing	 Â can	 Â help	 Â selection,	 Â because	 Â it	 Â 
can	 Â help	 Â us	 Â quickly	 Â find	 Â out	 Â where	 Â our	 Â 
intended	 Â result	 Â should	 Â locate.	 Â But	 Â it	 Â 
cannot	 Â help	 Â on	 Â range	 Â query.	 Â 
2) Hashing	 Â cannot	 Â help	 Â project,	 Â because	 Â 
it	 Â has	 Â nothing	 Â to	 Â do	 Â with	 Â selecting	 Â a	 Â 
specific	 Â attribute.	 Â 	 Â 
3) It	 Â can	 Â also	 Â help	 Â set	 Â operations	 Â 
because	 Â we	 Â need	 Â only	 Â look	 Â at	 Â the	 Â 
same	 Â bucket.	 Â 	 Â 
4) For	 Â the	 Â rest,	 Â please	 Â see	 Â below.	 Â 
â€¢ Recall:	 Â 	 Â Î´(R)	 Â = duplicate	 Â elimination	 Â 
â€¢ Step	 Â 1.	 Â Partition	 Â R	 Â into	 Â buckets
â€¢ Step	 Â 2.	 Â Apply	 Â Î´ to	 Â each	 Â bucket	 Â (may	 Â read	 Â in	 Â main	 Â memory)
â€¢ Cost:	 Â 3B(R)
â€¢ Assumption:B(R)	 Â <=	 Â M2
Hash	 Â Based	 Â Algorithms	 Â for	 Â 	 Â Î´
29 	 Â 
Hashing	 Â can	 Â help	 Â duplicate	 Â elimination,	 Â 
because	 Â with	 Â the	 Â same	 Â value	 Â hashing	 Â to	 Â 
the	 Â same	 Â bucket,	 Â we	 Â can	 Â easily	 Â do	 Â the	 Â 
elimination	 Â inside	 Â the	 Â bucket.	 Â 
â€¢Recall:	 Â 	 Â Î³(R)	 Â = grouping	 Â and	 Â aggregation
â€¢Step	 Â 1.	 Â Partition	 Â R	 Â into	 Â buckets
â€¢Step	 Â 2.	 Â Apply	 Â Î³ to	 Â each	 Â bucket	 Â (may	 Â read	 Â in	 Â main	 Â 
memory)
â€¢Cost:	 Â 3B(R)
â€¢Assumption:	 Â B(R)	 Â <=	 Â M2
Hash	 Â Based	 Â Algorithms	 Â for	 Â 	 Â Î³
30 	 Â 
Hashing	 Â can	 Â help	 Â for	 Â grouping	 Â and	 Â 
aggregation,	 Â because	 Â we	 Â first	 Â do	 Â hashing	 Â 
on	 Â the	 Â grouping	 Â attribute,	 Â and	 Â do	 Â the	 Â 
aggregation	 Â inside	 Â each	 Â bucket.	 Â 
ğ‘… â‹ˆ ğ‘†
â€¢Simple	 Â version:	 Â main	 Â memory	 Â hash-Â­â€based	 Â join
â€¢ Scan	 Â S,	 Â build	 Â buckets	 Â in	 Â main	 Â memory
â€¢ Then	 Â scan	 Â R	 Â and	 Â join
â€¢Requirement:	 Â min(B(R),	 Â B(S))	 Â <=	 Â M
Hash-Â­â€based	 Â Join
31 	 Â 
Take	 Â the	 Â matching	 Â attributes	 Â as	 Â the	 Â input	 Â 
of	 Â hash	 Â function.	 Â 
Build	 Â the	 Â hash	 Â buckets	 Â of	 Â smaller	 Â relation	 Â 
S	 Â into	 Â the	 Â main	 Â memory.	 Â For	 Â each	 Â tuple	 Â 
of	 Â R	 Â in	 Â bucket	 Â Ri,	 Â the	 Â tuples	 Â that	 Â it	 Â can	 Â join	 Â 
with	 Â must	 Â be	 Â in	 Â Si.	 Â 
	 Â 
ğ‘… â‹ˆ ğ‘†
â€¢Step	 Â 1:
â€¢ Hash	 Â S	 Â into	 Â M	 Â buckets
â€¢ send	 Â all	 Â buckets	 Â to	 Â disk
â€¢Step	 Â 2
â€¢ Hash	 Â R	 Â into	 Â M	 Â buckets
â€¢ Send	 Â all	 Â buckets	 Â to	 Â disk
â€¢Step	 Â 3
â€¢ Join	 Â every	 Â pair	 Â of	 Â buckets
Partitioned	 Â Hash	 Â Join
32 	 Â 
In	 Â the	 Â join	 Â part,	 Â we	 Â take	 Â the	 Â whole	 Â 
buckets	 Â Ri	 Â 	 Â and	 Â Si	 Â then	 Â join	 Â the	 Â tuples	 Â 
inside	 Â these	 Â two	 Â buckets.	 Â 
The	 Â cost	 Â is	 Â 3B(R)+3B(S).	 Â 
The	 Â size	 Â requirement	 Â is	 Â min(B(R),B(S)) Â â‰¤ğ‘€!.	 Â 
â€¢ Partition	 Â both	 Â 
relations	 Â using	 Â hash	 Â fn
h:	 Â 	 Â R	 Â tuples	 Â in	 Â partition	 Â 
i will	 Â only	 Â match	 Â S	 Â 
tuples	 Â in	 Â partition	 Â i.
â€¢ Read	 Â in	 Â a	 Â partition	 Â of	 Â 
R,	 Â hash	 Â it	 Â using	 Â h2	 Â (<>	 Â 
h!).	 Â Scan	 Â matching	 Â 
partition	 Â of	 Â S,	 Â search	 Â 
for	 Â matches.
Partitioned	 Â Hash-Â­â€Join
Partitions
of R & S
Input buffer
for Ri
Hash table for partition
Si ( < M-1 pages)
B main memory buffersDisk
Output 
buffer
Disk
Join Result
hash
fn
h2
h2
B main memory buffers DiskDisk
Original 
Relation OUTPUT
2INPUT
1
hash
function
h M-1
Partitions
1
2
M-1
. . .
	 Â 
A	 Â graph	 Â explanation	 Â of	 Â the	 Â above.	 Â 	 Â 
â€¢Cost:	 Â 3B(R)	 Â +	 Â 3B(S)
â€¢Assumption:	 Â At	 Â least	 Â one	 Â full	 Â bucket	 Â of	 Â the	 Â smaller	 Â 
rel must	 Â fit	 Â in	 Â memory:	 Â min(B(R),	 Â B(S))	 Â <=	 Â M2
Partitioned	 Â Hash	 Â Join
34 	 Â 
In	 Â here,	 Â because	 Â we	 Â assume	 Â that	 Â at	 Â least	 Â 
one	 Â full	 Â bucket	 Â of	 Â the	 Â two	 Â relations	 Â has	 Â fit	 Â 
into	 Â main	 Â memory.	 Â We	 Â thus	 Â have	 Â 
min(B(R)/M,	 Â B(S)/M)<=M	 Â 	 Â =>	 Â 
min(B(R),B(S))<=M2.	 Â 
	 Â 
â€¢ In	 Â a	 Â clustered	 Â index	 Â all	 Â tuples	 Â with	 Â the	 Â same	 Â value	 Â 
of	 Â the	 Â key	 Â are	 Â clustered	 Â on	 Â as	 Â few	 Â blocks	 Â as	 Â 
possible.
Indexed	 Â Based	 Â Algorithms
35
a a a a a a a a a a       
	 Â 
Index	 Â can	 Â help	 Â us	 Â directly	 Â go	 Â to	 Â the	 Â part	 Â 
that	 Â we	 Â are	 Â interested	 Â in.	 Â 
â€¢Selection	 Â on	 Â equality:	 Â Ïƒa=v(R)
â€¢Clustered	 Â index	 Â on	 Â a:	 Â 	 Â cost	 Â B(R)/V(R,a)
â€¢Unclustered index	 Â on	 Â a:	 Â cost	 Â T(R)/V(R,a)
Index	 Â Based	 Â Selection
36 	 Â 
B(R)	 Â means	 Â the	 Â block	 Â number	 Â of	 Â R;	 Â 
V(R,	 Â a)	 Â means	 Â the	 Â number	 Â of	 Â distinct	 Â 
tuples	 Â in	 Â attribute	 Â a	 Â of	 Â R.	 Â 	 Â 
T(R)	 Â means	 Â the	 Â number	 Â of	 Â tuples	 Â in	 Â R.	 Â 
The	 Â cost	 Â of	 Â using	 Â clustered	 Â index	 Â can	 Â be	 Â 
approximated	 Â by	 Â the	 Â average	 Â number	 Â of	 Â 
blocks	 Â for	 Â a	 Â certain	 Â value.	 Â 
Similarly,	 Â The	 Â cost	 Â of	 Â using	 Â unclustered	 Â 
index	 Â can	 Â be	 Â approximated	 Â by	 Â the	 Â 
average	 Â number	 Â of	 Â tuples	 Â for	 Â a	 Â certain	 Â 
value.	 Â 
Thatâ€™s	 Â the	 Â cost	 Â of	 Â simply	 Â read	 Â the	 Â data.	 Â 
â€¢Example:	 Â B(R)	 Â =	 Â 2000,	 Â 	 Â T(R)	 Â =	 Â 100,000,	 Â V(R,	 Â a)	 Â =	 Â 20,	 Â 
compute	 Â the	 Â cost	 Â of	 Â Ïƒa=v(R)
â€¢Cost	 Â of	 Â table	 Â scan:
â€¢ If	 Â R	 Â is	 Â clustered:	 Â B(R)	 Â =	 Â 2000	 Â I/Os
â€¢ If	 Â R	 Â is	 Â unclustered:	 Â T(R)	 Â =	 Â 100,000	 Â I/Os
â€¢Cost	 Â of	 Â index	 Â based	 Â selection:
â€¢ If	 Â index	 Â is	 Â clustered:	 Â B(R)/V(R,a)	 Â =	 Â 100
â€¢ If	 Â index	 Â is	 Â unclustered:	 Â T(R)/V(R,a)	 Â =	 Â 5000
â€¢Notice:	 Â when	 Â V(R,a)	 Â is	 Â small,	 Â then	 Â unclustered	 Â index	 Â 
is	 Â useless
Index	 Â Based	 Â Selection
37 	 Â 
When	 Â V(R,a)	 Â is	 Â small,	 Â the	 Â cost	 Â of	 Â using	 Â an	 Â 
unclustered	 Â index	 Â will	 Â be	 Â similar	 Â to	 Â simply	 Â 
scanning	 Â through	 Â the	 Â table	 Â without	 Â any	 Â 
index.	 Â 
ğ‘… Â  â‹ˆ ğ‘†
â€¢ Assume	 Â S	 Â has	 Â an	 Â index	 Â on	 Â the	 Â join	 Â attribute
â€¢ Iterate over	 Â R,	 Â for	 Â each	 Â tuple	 Â fetch	 Â corresponding	 Â tuple(s)	 Â 
from	 Â S
â€¢ Assume	 Â R	 Â is	 Â clustered.	 Â Cost:
â€¢ If	 Â index	 Â is	 Â clustered:	 Â 	 Â B(R)	 Â +	 Â T(R)B(S)/V(S,a)
â€¢ If	 Â index	 Â is	 Â unclustered:	 Â B(R)	 Â +	 Â T(R)T(S)/V(S,a)
Index	 Â Based	 Â Join
38 	 Â 
Assume	 Â that	 Â S	 Â has	 Â an	 Â index	 Â on	 Â the	 Â join	 Â 
attribute.	 Â Then	 Â we	 Â need	 Â to	 Â iterate	 Â over	 Â R	 Â 
and	 Â find	 Â the	 Â corresponding	 Â tuples	 Â in	 Â S	 Â 
that	 Â satisfy	 Â the	 Â join	 Â condition.	 Â Then	 Â 
output	 Â that	 Â to	 Â file.	 Â 	 Â 
Because	 Â we	 Â need	 Â to	 Â read	 Â R	 Â from	 Â disk,	 Â we	 Â 
need	 Â B(R),	 Â and	 Â we	 Â iterate	 Â through	 Â R,	 Â 
thatâ€™s	 Â T(R)	 Â times.	 Â Then	 Â the	 Â finding	 Â of	 Â 
tuples	 Â in	 Â S	 Â will	 Â have	 Â B(S)/V(s,a)	 Â or	 Â 
T(S)/V(S,a)	 Â based	 Â on	 Â whether	 Â it	 Â is	 Â 
clustered	 Â or	 Â not.	 Â 	 Â 
â€¢Assume	 Â both	 Â R	 Â and	 Â S	 Â have	 Â a	 Â sorted	 Â index	 Â (B+	 Â tree)	 Â 
on	 Â the	 Â join	 Â attribute
â€¢Then	 Â perform	 Â a	 Â merge	 Â join	 Â (called	 Â zig-Â­â€zag join)
â€¢Cost:	 Â B(R)	 Â +	 Â B(S)
Index	 Â Based	 Â Join
39 	 Â 
If	 Â the	 Â indexes	 Â on	 Â the	 Â join	 Â attribute	 Â are	 Â 
sorted,	 Â then	 Â we	 Â need	 Â to	 Â perform	 Â only	 Â the	 Â 
final	 Â step	 Â of	 Â the	 Â simple	 Â sort-Â­â€based	 Â join.	 Â 
This	 Â method	 Â is	 Â called	 Â zig-Â­â€zag	 Â join,	 Â because	 Â 
we	 Â jump	 Â back	 Â and	 Â forth	 Â between	 Â the	 Â 
indexes	 Â finding	 Â join	 Â values	 Â that	 Â they	 Â share	 Â 
in	 Â common.	 Â 	 Â 
	 Â 
