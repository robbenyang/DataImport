1 
Wri$ng	  Cache	  Friendly	  Code	  
  Make	  the	  common	  case	  go	  fast	  
  Focus	  on	  the	  inner	  loops	  of	  the	  core	  func0ons	  
  Minimize	  the	  misses	  in	  the	  inner	  loops	  
  Repeated	  references	  to	  variables	  are	  good	  (temporal	  locality)	  
  Stride-­‐1	  reference	  pa>erns	  are	  good	  (spa0al	  locality)	  
Key	  idea:	  Our	  qualita$ve	  no$on	  of	  locality	  is	  quan$fied	  
through	  our	  understanding	  of	  cache	  memories.	  
2 
Today	  
  Performance	  impact	  of	  caches	  
  The	  memory	  mountain	  
  Rearranging	  loops	  to	  improve	  spa0al	  locality	  
  Using	  blocking	  to	  improve	  temporal	  locality	  
	  
3 
The	  Memory	  Mountain	  
  Read	  throughput	  (read	  bandwidth)	  
  Number	  of	  bytes	  read	  from	  memory	  per	  second	  (MB/s)	  
	  
  Memory	  mountain:	  Measured	  read	  throughput	  as	  a	  
func$on	  of	  spa$al	  and	  temporal	  locality.	  
  Compact	  way	  to	  characterize	  memory	  system	  performance.	  	  
4 
Memory	  Mountain	  Test	  Func$on	  
/* The test function */ 
void test(int elems, int stride) { 
    int i, result = 0;  
    volatile int sink;  
 
    for (i = 0; i < elems; i += stride) 
 result += data[i]; 
    sink = result; /* So compiler doesn't optimize away the loop */ 
} 
 
/* Run test(elems, stride) and return read throughput (MB/s) */ 
double run(int size, int stride, double Mhz) 
{ 
    double cycles; 
    int elems = size / sizeof(int);  
 
    test(elems, stride);                     /* warm up the cache */ 
    cycles = fcyc2(test, elems, stride, 0);  /* call test(elems,stride) */ 
    return (size / stride) / (cycles / Mhz); /* convert cycles to MB/s */ 
} 
 
5 
The	  Memory	  Mountain	  
64
M
 8M
 1M
 
12
8K
 
16
K
 2K
 0 
1000 
2000 
3000 
4000 
5000 
6000 
7000 
s1
 
s3
 
s5
 
s7
 
s9
 
s1
1 
s1
3 
s1
5 
s3
2 Working set size (bytes) 
R
ea
d 
 th
ro
ug
hp
ut
 (M
B
/s
) 
Stride (x8 bytes) 
Intel	  Core	  i7	  
32	  KB	  L1	  	  i-­‐cache	  
32	  KB	  L1	  d-­‐cache	  
256	  KB	  unified	  L2	  cache	  
8M	  unified	  L3	  cache	  
	  
All	  caches	  on-­‐chip	  
6 
The	  Memory	  Mountain	  
64
M
 8M
 1M
 
12
8K
 
16
K
 2K
 0 
1000 
2000 
3000 
4000 
5000 
6000 
7000 
s1
 
s3
 
s5
 
s7
 
s9
 
s1
1 
s1
3 
s1
5 
s3
2 Working set size (bytes) 
R
ea
d 
 th
ro
ug
hp
ut
 (M
B
/s
) 
Stride (x8 bytes) 
Intel	  Core	  i7	  
32	  KB	  L1	  	  i-­‐cache	  
32	  KB	  L1	  d-­‐cache	  
256	  KB	  unified	  L2	  cache	  
8M	  unified	  L3	  cache	  
	  
All	  caches	  on-­‐chip	  
Slopes	  of	  
spa*al	  	  
locality	  
7 
The	  Memory	  Mountain	  
64
M
 8M
 1M
 
12
8K
 
16
K
 2K
 0 
1000 
2000 
3000 
4000 
5000 
6000 
7000 
s1
 
s3
 
s5
 
s7
 
s9
 
s1
1 
s1
3 
s1
5 
s3
2 Working set size (bytes) 
R
ea
d 
 th
ro
ug
hp
ut
 (M
B
/s
) 
Stride (x8 bytes) 
L1"
L2"
Mem"
L3"
Intel	  Core	  i7	  
32	  KB	  L1	  	  i-­‐cache	  
32	  KB	  L1	  d-­‐cache	  
256	  KB	  unified	  L2	  cache	  
8M	  unified	  L3	  cache	  
	  
All	  caches	  on-­‐chip	  
Slopes	  of	  
spa*al	  	  
locality	  
Ridges	  of	  	  	  
Temporal	  
	  locality	  
8 
Today	  
  Performance	  impact	  of	  caches	  
  The	  memory	  mountain	  
  Rearranging	  loops	  to	  improve	  spa0al	  locality	  
  Using	  blocking	  to	  improve	  temporal	  locality	  
	  
9 
Miss	  Rate	  Analysis	  for	  Matrix	  Mul$ply	  
  Assume:	  
  Line	  size	  =	  32B	  (big	  enough	  for	  four	  64-­‐bit	  words)	  
  Matrix	  dimension	  (N)	  is	  very	  large	  
  Approximate	  1/N	  as	  0.0	  
  Cache	  is	  not	  even	  big	  enough	  to	  hold	  mul0ple	  rows	  
  Analysis	  Method:	  
  Look	  at	  access	  pa>ern	  of	  inner	  loop	  
A 
k 
i 
B 
k 
j 
C 
i 
j 
10 
Matrix	  Mul$plica$on	  Example	  
  Descrip$on:	  
  Mul0ply	  N	  x	  N	  matrices	  
  O(N3)	  total	  opera0ons	  
  N	  reads	  per	  source	  
element	  
  N	  values	  summed	  per	  
des0na0on	  
  but	  may	  be	  able	  to	  
hold	  in	  register	  
/* ijk */ 
for (i=0; i<n; i++)  { 
  for (j=0; j<n; j++) { 
    sum = 0.0; 
    for (k=0; k<n; k++)  
      sum += a[i][k] * b[k][j]; 
    c[i][j] = sum; 
  } 
}  
Variable sum 
held in register 
11 
Layout	  of	  C	  Arrays	  in	  Memory	  
  C	  arrays	  allocated	  in	  row-­‐major	  order	  
  each	  row	  in	  con0guous	  memory	  loca0ons	  
  Stepping	  through	  columns	  in	  one	  row:	  
  for (i = 0; i < N; i++) 
sum += a[0][i]; 
  accesses	  successive	  elements	  
  if	  block	  size	  (B)	  >	  4	  bytes,	  exploit	  spa0al	  locality	  
  compulsory	  miss	  rate	  =	  4	  bytes	  /	  B	  
  Stepping	  through	  rows	  in	  one	  column:	  
  for (i = 0; i < n; i++) 
sum += a[i][0]; 
  accesses	  distant	  elements	  
  no	  spa0al	  locality!	  
  compulsory	  miss	  rate	  =	  1	  (i.e.	  100%)	  
12 
Matrix	  Mul$plica$on	  (ijk)	  
/* ijk */ 
for (i=0; i<n; i++)  { 
  for (j=0; j<n; j++) { 
    sum = 0.0; 
    for (k=0; k<n; k++)  
      sum += a[i][k] * b[k][j]; 
    c[i][j] = sum; 
  } 
}  
A	   B	   C	  
(i,*)	  
(*,j)	  
(i,j)	  
Inner	  loop:	  
Column-­‐	  
wise	  
Row-­‐wise	   Fixed	  
Misses	  per	  inner	  loop	  itera0on:	  
	   	  A 	  B 	  C	  
	   	  0.25 	  1.0 	  0.0	  
13 
Matrix	  Mul$plica$on	  (jik)	  
/* jik */ 
for (j=0; j<n; j++) { 
  for (i=0; i<n; i++) { 
    sum = 0.0; 
    for (k=0; k<n; k++) 
      sum += a[i][k] * b[k][j]; 
    c[i][j] = sum 
  } 
} 
A	   B	   C	  
(i,*)	  
(*,j)	  
(i,j)	  
Inner	  loop:	  
Row-­‐wise	   Column-­‐	  
wise	  
Fixed	  
Misses	  per	  inner	  loop	  itera0on:	  
	   	  A 	  B 	  C	  
a)  0	  
b)  .25	  
c)  .75	  
d)  1.0	  
e)  2.0	  
14 
Matrix	  Mul$plica$on	  (jik)	  
/* jik */ 
for (j=0; j<n; j++) { 
  for (i=0; i<n; i++) { 
    sum = 0.0; 
    for (k=0; k<n; k++) 
      sum += a[i][k] * b[k][j]; 
    c[i][j] = sum 
  } 
} 
A	   B	   C	  
(i,*)	  
(*,j)	  
(i,j)	  
Inner	  loop:	  
Row-­‐wise	   Column-­‐	  
wise	  
Fixed	  
Misses	  per	  inner	  loop	  itera0on:	  
	   	  A 	  B 	  C	  
	   	  0.25 	  1.0 	  0.0	  
15 
Matrix	  Mul$plica$on	  (kij)	  
/* kij */ 
for (k=0; k<n; k++) { 
  for (i=0; i<n; i++) { 
    r = a[i][k]; 
    for (j=0; j<n; j++) 
      c[i][j] += r * b[k][j]; 
  } 
} 
 
A	   B	   C	  
(i,*)	  
(i,k)	   (k,*)	  
Inner	  loop:	  
Row-­‐wise	   Row-­‐wise	  Fixed	  
Misses	  per	  inner	  loop	  itera0on:	  
	   	  A 	  B 	  C	  
a)  0	  
b)  .25	  
c)  .75	  
d)  1.0	  
e)  2.0	  
16 
Matrix	  Mul$plica$on	  (kij)	  
/* kij */ 
for (k=0; k<n; k++) { 
  for (i=0; i<n; i++) { 
    r = a[i][k]; 
    for (j=0; j<n; j++) 
      c[i][j] += r * b[k][j]; 
  } 
} 
 
A	   B	   C	  
(i,*)	  
(i,k)	   (k,*)	  
Inner	  loop:	  
Row-­‐wise	   Row-­‐wise	  Fixed	  
Misses	  per	  inner	  loop	  itera0on:	  
	   	  A 	  B 	  C	  
	   	  0.0 	  0.25 	  0.25	  
17 
Matrix	  Mul$plica$on	  (ikj)	  
/* ikj */ 
for (i=0; i<n; i++) { 
  for (k=0; k<n; k++) { 
    r = a[i][k]; 
    for (j=0; j<n; j++) 
      c[i][j] += r * b[k][j]; 
  } 
} 
A	   B	   C	  
(i,*)	  
(i,k)	   (k,*)	  
Inner	  loop:	  
Row-­‐wise	   Row-­‐wise	  Fixed	  
Misses	  per	  inner	  loop	  itera0on:	  
	   	  A 	  B 	  C	  
	   	  0.0 	  0.25 	  0.25	  
18 
Matrix	  Mul$plica$on	  (jki)	  
/* jki */ 
for (j=0; j<n; j++) { 
  for (k=0; k<n; k++) { 
    r = b[k][j]; 
    for (i=0; i<n; i++) 
      c[i][j] += a[i][k] * r; 
  } 
}   
A	   B	   C	  
(*,j)	  
(k,j)	  
Inner	  loop:	  
(*,k)	  
Column-­‐	  
wise	  
Column-­‐	  
wise	  
Fixed	  
Misses	  per	  inner	  loop	  itera0on:	  
	   	  A 	  B 	  C	  
a)  0	  
b)  .25	  
c)  .75	  
d)  1.0	  
e)  2.0	  
19 
Matrix	  Mul$plica$on	  (jki)	  
/* jki */ 
for (j=0; j<n; j++) { 
  for (k=0; k<n; k++) { 
    r = b[k][j]; 
    for (i=0; i<n; i++) 
      c[i][j] += a[i][k] * r; 
  } 
}   
A	   B	   C	  
(*,j)	  
(k,j)	  
Inner	  loop:	  
(*,k)	  
Column-­‐	  
wise	  
Column-­‐	  
wise	  
Fixed	  
Misses	  per	  inner	  loop	  itera0on:	  
	   	  A 	  B 	  C	  
	   	  1.0 	  0.0 	  1.0	  
20 
Matrix	  Mul$plica$on	  (kji)	  
/* kji */ 
for (k=0; k<n; k++) { 
  for (j=0; j<n; j++) { 
    r = b[k][j]; 
    for (i=0; i<n; i++) 
      c[i][j] += a[i][k] * r; 
  } 
}   
A	   B	   C	  
(*,j)	  
(k,j)	  
Inner	  loop:	  
(*,k)	  
Fixed	  Column-­‐	  
wise	  
Column-­‐	  
wise	  
Misses	  per	  inner	  loop	  itera0on:	  
	   	  A 	  B 	  C	  
	   	  1.0 	  0.0 	  1.0	  
21 
Summary	  of	  Matrix	  Mul$plica$on	  
ijk	  (&	  jik):	  	  
• 	  2	  loads,	  0	  stores	  
• 	  misses/iter	  =	  1.25	  
kij	  (&	  ikj):	  	  
• 	  2	  loads,	  1	  store	  
• 	  misses/iter	  =	  0.5	  
jki	  (&	  kji):	  	  
• 	  2	  loads,	  1	  store	  
• 	  misses/iter	  =	  2.0	  
for (i=0; i<n; i++) { 
  for (j=0; j<n; j++) { 
   sum = 0.0; 
   for (k=0; k<n; k++)  
     sum += a[i][k] * b[k][j]; 
   c[i][j] = sum; 
 } 
}  
for (k=0; k<n; k++) { 
 for (i=0; i<n; i++) { 
  r = a[i][k]; 
  for (j=0; j<n; j++) 
   c[i][j] += r * b[k][j];    
 } 
} 
for (j=0; j<n; j++) { 
 for (k=0; k<n; k++) { 
   r = b[k][j]; 
   for (i=0; i<n; i++) 
    c[i][j] += a[i][k] * r; 
 } 
} 
22 
Core	  i7	  Matrix	  Mul$ply	  Performance	  
0 
10 
20 
30 
40 
50 
60 
50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 
C
yc
le
s 
pe
r i
nn
er
 lo
op
 it
er
at
io
n 
Array size (n) 
jki 
kji 
ijk 
jik 
kij 
ikj 
jki	  /	  kji	  
ijk	  /	  jik	  
kij	  /	  ikj	  
23 
0 
10 
20 
30 
40 
50 
60 
50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 
C
yc
le
s 
pe
r i
nn
er
 lo
op
 it
er
at
io
n 
Array size (n) 
for (i=0; i<n; i++) { 
  for (j=0; j<n; j++) { 
   sum = 0.0; 
   for (k=0; k<n; k++)  
     sum += a[i][k] * b[k][j]; 
   c[i][j] = sum; 
 } 
}  
for (k=0; k<n; k++) { 
 for (i=0; i<n; i++) { 
  r = a[i][k]; 
 for (j=0; j<n; j++) 
   c[i][j] += r * b[k][j];    
 } 
} 
for (j=0; j<n; j++) { 
 for (k=0; k<n; k++) { 
   r = b[k][j]; 
   for (i=0; i<n; i++) 
    c[i][j] += a[i][k] * r; 
 } 
} 
From	  where	  comes	  the	  performance?	  
ijk	  (&	  jik):	  	  
• 	  2	  loads,	  0	  stores	  
• 	  misses/iter	  =	  1.25	  
kij	  (&	  ikj):	  	  
• 	  2	  loads,	  1	  store	  
• 	  misses/iter	  =	  0.5	  
jki	  (&	  kji):	  	  
• 	  2	  loads,	  1	  store	  
• 	  misses/iter	  =	  2.0	  
a)  Spa$al	  locality	  
b)  Temporal	  locality	  
24 
Today	  
  Performance	  impact	  of	  caches	  
  The	  memory	  mountain	  
  Rearranging	  loops	  to	  improve	  spa0al	  locality	  
  Using	  blocking	  to	  improve	  temporal	  locality	  
	  
25 
26 
Example:	  Matrix	  Mul$plica$on	  
a b 
i	  
j	  
*	  
c 
=	  
c = (double *) calloc(sizeof(double), n*n); 
 
/* Multiply n x n matrices a and b  */ 
void mmm(double *a, double *b, double *c, int n) { 
    int i, j, k; 
    for (i = 0; i < n; i++) 
 for (j = 0; j < n; j++) 
             for (k = 0; k < n; k++) 
     c[i*n+j] += a[i*n + k]*b[k*n + j]; 
} 
27 
Cache	  Miss	  Analysis	  
  Assume:	  	  
  Matrix	  elements	  are	  doubles	  
  Cache	  block	  =	  8	  doubles	  
  Cache	  size	  C	  <<	  n	  (much	  smaller	  than	  n)	  
  First	  itera$on:	  
  n/8	  +	  n	  =	  9n/8	  misses	  
  Aberwards	  in	  cache:	  
(schema0c)	  
*	  =	  
n	  
*	  =	  
8	  wide	  
28 
Cache	  Miss	  Analysis	  
  Assume:	  	  
  Matrix	  elements	  are	  doubles	  
  Cache	  block	  =	  8	  doubles	  
  Cache	  size	  C	  <<	  n	  (much	  smaller	  than	  n)	  
  Second	  itera$on:	  
  Again:	  
n/8	  +	  n	  =	  9n/8	  misses	  
  Total	  misses:	  
  9n/8	  *	  n2	  =	  (9/8)	  *	  n3	  	  
n	  
*	  =	  
8	  wide	  
29 
Blocked	  Matrix	  Mul$plica$on	  
c = (double *) calloc(sizeof(double), n*n); 
 
/* Multiply n x n matrices a and b  */ 
void mmm(double *a, double *b, double *c, int n) { 
    int i, j, k; 
    for (i = 0; i < n; i+=B) 
 for (j = 0; j < n; j+=B) 
             for (k = 0; k < n; k+=B) 
   /* B x B mini matrix multiplications */ 
                  for (i1 = i; i1 < i+B; i1++) 
                      for (j1 = j; j1 < j+B; j1++) 
                          for (k1 = k; k1 < k+B; k1++) 
                       c[i1*n+j1] += a[i1*n + k1]*b[k1*n + j1]; 
} 
a b 
i1	  
j1	  
*	  
c 
=	  
c 
+	  
Block	  size	  B	  x	  B	  
30 
Cache	  Miss	  Analysis	  
  Assume:	  	  
  Cache	  block	  =	  8	  doubles	  
  Cache	  size	  C	  <<	  n	  (much	  smaller	  than	  n)	  
  Three	  blocks	  	  	  	  	  	  	  fit	  into	  cache:	  3B2	  <	  C	  
  First	  (block)	  itera$on:	  
  B2/8	  misses	  for	  each	  block	  
  2n/B	  *	  B2/8	  =	  nB/4	  
(omidng	  matrix	  c)	  
  Aberwards	  in	  cache	  
(schema0c)	  
*	  =	  
*	  =	  
Block	  size	  B	  x	  B	  
n/B	  blocks	  
31 
Cache	  Miss	  Analysis	  
  Assume:	  	  
  Cache	  block	  =	  8	  doubles	  
  Cache	  size	  C	  <<	  n	  (much	  smaller	  than	  n)	  
  Three	  blocks	  	  	  	  	  	  	  fit	  into	  cache:	  3B2	  <	  C	  
  Second	  (block)	  itera$on:	  
  Same	  as	  first	  itera0on	  
  2n/B	  *	  B2/8	  =	  nB/4	  
	  
  Total	  misses:	  
  nB/4	  *	  (n/B)2	  =	  n3/(4B)	  
*	  =	  
Block	  size	  B	  x	  B	  
n/B	  blocks	  
32 
Summary	  
  No	  blocking:	  (9/8)	  *	  n3	  
  Blocking:	  1/(4B)	  *	  n3	  
  Suggest	  largest	  possible	  block	  size	  B,	  but	  limit	  3B2	  <	  C!	  
  Reason	  for	  drama$c	  difference:	  
  Matrix	  mul0plica0on	  has	  inherent	  temporal	  locality:	  
  Input	  data:	  3n2,	  computa0on	  2n3	  
  Every	  array	  elements	  used	  O(n)	  0mes!	  
  But	  program	  has	  to	  be	  wri>en	  properly	  
33 
Concluding	  Observa$ons	  
  Programmer	  can	  op$mize	  for	  cache	  performance	  
  How	  data	  structures	  are	  organized	  
  How	  data	  are	  accessed	  
  Nested	  loop	  structure	  
  Blocking	  is	  a	  general	  technique	  
  All	  systems	  favor	  “cache	  friendly	  code”	  
  Gedng	  absolute	  op0mum	  performance	  is	  very	  plaiorm	  specific	  
  Cache	  sizes,	  line	  sizes,	  associa0vi0es,	  etc.	  
  Can	  get	  most	  of	  the	  advantage	  with	  generic	  code	  
  Keep	  working	  set	  reasonably	  small	  (temporal	  locality)	  
  Use	  small	  strides	  (spa0al	  locality)	  
