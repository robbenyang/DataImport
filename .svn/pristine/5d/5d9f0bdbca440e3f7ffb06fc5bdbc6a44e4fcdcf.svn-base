Program Optimization 
Through Loop Vectorization 
Topics covered 
â€¢â€¯ What are the microprocessor vector extensions 
or SIMD (Single Instruction Multiple Data Units) 
2 
	

â€¢â€¯ Overcoming limitations to SIMD-Vectorization 
â€“â€¯ Data Dependences 
â€“â€¯ Data Alignment 
â€“â€¯ Aliasing 
â€“â€¯ Non-unit strides 
â€“â€¯ Conditional Statements 
â€¢â€¯ Vectorization with intrinsics  
Simple Example 
â€¢â€¯ Loop vectorization transforms a program so that the 
same operation is performed at the same time on several 
vector elements 
for (i=0; i<n; i++)  
  c[i] = a[i] + b[i]; 
â€¦ Register File 
X1 
Y1 
Z1 
32 bits 
32 bits 
+ 
32 
bits 
Scalar Unit Vector 
Unit 
lw $t0, 0($a0) 
lw  $t1, 0($a1) 
add $t3, $t0, $t1 
sw $t3, 0($a3) 
  
lwv $vt0, 0($a0) 
lwv $vt1, 0($a1) 
addv $vt3, $vt0, $vt1 
swv %vt3, $0($a3) 
3 
SIMD Vectorization 
â€¢â€¯ The use of SIMD units can speed up the program.  
â€¢â€¯ Intel SSE and IBM Altivec have 128-bit vector registers and 
functional units 
â€“â€¯ 4 32-bit single precision floating point numbers 
â€“â€¯ 2 64-bit double precision floating point numbers 
â€“â€¯ 4 32-bit integer numbers 
â€“â€¯ 2 64 bit integer 
â€“â€¯ 8 16-bit integer or shorts 
â€“â€¯ 16 8-bit bytes or chars 
â€¢â€¯ Assuming a single ALU, these SIMD units can execute 4 single 
precision floating point number or 2 double precision operations in 
the time it takes to do only one of these operations by a scalar unit. 
â€¢â€¯ Newer processors, such as Sandy or Ivy Bridge have AVX that 
support 256-bit vector registers. 
 
	

4 
Experimental results  
â€¢â€¯ Results are shown for  different platforms with 
their compilers: 
â€“â€¯ Report generated by the compiler 
â€“â€¯ Execution Time for each platform 
 
  
5 
Platform 2: IBM Power 7 
IBM Power 7, 3.55 GHz 
IBM xlc compiler, version 11.0 
OS Red Hat Linux Enterprise 5.4 
Platform 1: Intel Nehalem  
Intel Core i7 CPU 920@2.67GHz 
Intel ICC compiler, version 11.1 
OS Ubuntu Linux 9.04 
The examples use single precision floating point numbers  
 
Executing Our Simple Example 
for (i=0; i<n; i++)  
  c[i] = a[i] + b[i]; 
â€¦ Register File 
X
1 
Y1 
Z1 
32 bits 
+ 
32 bits 
Scalar Unit Vector 
Unit 
Intel Nehalem 
Exec. Time scalar code: 6.1  
Exec. Time vector code: 3.2  
Speedup: 1.8 
 
IBM Power 7 
Exec. Time scalar code: 2.1 
Exec. Time vector code: 1.0  
Speedup: 2.1 
 
6 
How do we access the SIMD  units? 
â€¢â€¯ Three choices 
1.â€¯ Assembly Language 
 
 
2.â€¯ Macros or Vector Intrinsics 
3.â€¯ C code and a vectorizing compiler 
7 
for (i=0; i<LEN; i++)  
  c[i] = a[i] + b[i]; 
void example(){   
__m128 rA, rB, rC;  
 for (int i = 0; i <LEN; i+=4){     
    rA = _mm_load_ps(&a[i]);     
    rB = _mm_load_ps(&b[i]);     
    rC = _mm_add_ps(rA,rB);    
   _mm_store_ps(&C[i], rC);   
}} 
 
     ..B8.5  
 movaps    a(,%rdx,4), %xmm0 
 addps     b(,%rdx,4), %xmm0 
 movaps    %xmm0, c(,%rdx,4) 
 addq      $4, %rdx 
 cmpq      $rdi, %rdx 
 jl        ..B8.5 
Why use compiler vectorization? 
1.â€¯ Easier  
2.â€¯ Portable across vendors and machines 
â€“â€¯ Although compiler directives differ across compilers 
3.â€¯ Better performance of the compiler generated code  
â€“â€¯ Compiler applies other transformations 
8 
Compilers make your codes (almost) machine independent  
How well do compilers vectorize? 
                 Compiler 
Loops 
XLC ICC GCC 
Total 159 
  Vectorized 74 75 32 
  Not vectorized 85 84 127 
Average Speed Up 1.73 1.85 1.30 
9 
                 Compiler 
Loops 
XLC but 
not ICC 
ICC but 
not XLC 
Vectorized 25 26 
How well do compilers vectorize? 
                 Compiler 
Loops 
XLC ICC GCC 
Total 159 
  Vectorized 74 75 32 
  Not vectorized 85 84 127 
Average Speed Up 1.73 1.85 1.30 
10 
                 Compiler 
Loops 
XLC but 
not ICC 
ICC but 
not XLC 
Vectorized 25 26 
By adding manual vectorization the average speedup  
was 3.78 (versus 1.73 obtained by the XLC compiler)  
Compiler Vectorization 
â€¢â€¯ Compilers can vectorize for us, but they may fail: 
1.â€¯ Code cannot be vectorized due to data 
dependences: vectorization will produce incorrect 
results. 
 
2.â€¯ Code can be vectorized, but the compiler fail to 
vectorize the code in its current form 
1.â€¯ Programmer can use compiler directives to give the 
compiler the necessary information 
2.â€¯ Programmer can transform the code  
11 
Example 
12 
 
void test(float* A,float* B,float* C,float* D, float* E) 
{Â Â  Â  
  for (int i = 0; i <LEN; i++){Â Â  Â  Â  
   A[i]=B[i]+C[i]+D[i]+E[i]; 
Â Â } 
} 
Compiler directives 
13 
 
void test(float* A, float* B, float* 
C, float* D, float* E) 
{Â Â  Â  
  for (int i = 0; i <LEN; i++){Â Â  Â  Â  
   A[i]=B[i]+C[i]+D[i]+E[i]; 
Â Â } 
} 
Intel Nehalem 
Compiler report: Loop was not 
vectorized. 
Exec. Time scalar code: 5.6 
Exec. Time vector code: -- 
Speedup: -- 
Intel Nehalem 
Compiler report: Loop was 
vectorized. 
Exec. Time scalar code: 5.6 
Exec. Time vector code: 2.2 
Speedup: 2.5 
 
void test(float* __restrict__ A,  
float* __restrict__ B,  
float* __restrict__ C,  
float* __restrict__ D,  
float* __restrict__ E) 
{Â Â  Â  
  for (int i = 0; i <LEN; i++){Â Â  Â  Â  
   A[i]=B[i]+C[i]+D[i]+E[i]; 
Â Â } 
} 
Compiler directives 
14 
 
void test(float* A, float* B, float* 
C, float* D, float* E) 
{Â Â  Â  
  for (int i = 0; i <LEN; i++){Â Â  Â  Â  
   A[i]=B[i]+C[i]+D[i]+E[i]; 
Â Â } 
} 
Power 7 
Compiler report: Loop was not 
vectorized.  
Exec. Time scalar code: 2.3 
Exec. Time vector code: -- 
Speedup: -- 
Power 7 
Compiler report: Loop was 
vectorized. 
Exec. Time scalar code: 1.6 
Exec. Time vector code: 0.6 
Speedup: 2.7 
 
void test(float* __restrict__ A,  
float* __restrict__ B,  
float* __restrict__ C,  
float* __restrict__ D,  
float* __restrict__ E) 
{Â Â  Â  
  for (int i = 0; i <LEN; i++){Â Â  Â  Â  
   A[i]=B[i]+C[i]+D[i]+E[i]; 
Â Â } 
} 
Vectorization is not always legal 
â€¢â€¯ Vectorization of some codes could produce 
incorrect results 
â€¢â€¯ Compilers (and programmers) can compute data 
dependences to determine if a program can be 
vectorized 
 
15 
Definition of Dependence 
â€¢â€¯ A statement S is said to be data dependent on 
statement T if 
â€“â€¯ T executes before S in the original sequential/scalar program 
â€“â€¯ S and T access the same data item 
â€“â€¯ At least one of the accesses is a write.  
16 
Tour on Data Dependences 
Flow dependence (True dependence) 
Anti dependence 
Output dependence 
S1: X = A+B 
S2: C= X+A 
S1: A = X + B 
S2: X= C + D 
S1: X = A+B 
S2: X= C + D 
S1 
S2 
S1 
S2 
S1 
S2 
17 
Data Dependence 
 
â€¢â€¯ Dependences indicate an execution order that 
must be honored. 
â€¢â€¯ Executing statements in the order of the 
dependences guarantee correct results. 
â€¢â€¯ Statements not dependent on each other can be 
reordered, executed in parallel, or coalesced into 
a vector operation. 
18 
Data Dependences 
19 
S1: A = B + D 
S2: C = A + T 
S3: Z = P + T 
S1 
S2 
S3 
Data Dependences 
20 
S1: A = B + D 
S2: C = A + T 
S3: Z = P + T 
S1 
S2 
S3 
S1: A = B + D 
S3: Z = P + T 
S2: C = A + T 
S2: C = A + T 
S1: A = B + D 
S3: Z = P + T 
Dependences in Loops (I) 
â€¢â€¯ Dependences in loops are easy to understand if the loops are 
unrolled. Now the dependences are between statement â€œexecutionsâ€. 
for (i=0; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i] + 2; 
} 
S1 
21 
S2 
Dependences in Loops (I) 
for (i=0; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i] + 2; 
} 
S1 
S2 
i=0 
22 
 
 
S1: a[0] = b[0] + 1 
S2: c[0] = a[0] + 2 
 
 
 
S1: a[1] = b[1] + 1 
S2: c[1] = a[1] + 2 
 
 
 
S1: a[2] = b[2] + 1 
S2: c[2] = a[2] + 2 
 
i=1 i=2 
Dependences in Loops (I) 
for (i=0; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i] + 2; 
} 
S1 
S2 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
0 1 2 3 â€¦ 
â€¦ 
23 
Dependences in Loops (I) 
for (i=0; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i] + 2; 
} 
S1 
S2 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
0 1 2 3 â€¦ 
â€¦ 
24 
     Loop independent dependence 
Dependences in Loops (I) 
for (i=0; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i] + 2; 
} 
S1 
S2 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
0 1 2 3 â€¦ 
â€¦ 
25 
S1 
S2 
For the whole loop 
Dependences in Loops (I) 
for (i=0; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i] + 2; 
} 
S1 
S2 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
0 1 2 3 â€¦ 
â€¦ 
26 
S1 
S2 
For the whole loop 
0 
Dependences in Loops (I) 
for (i=0; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i] + 2; 
} 
S1 
S2 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
0 1 2 3 â€¦ 
â€¦ 
27 
S1 
S2 
For the whole loop 
0 
distance 
Dependences in Loops (I) 
for (i=0; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i] + 2; 
} 
S1 
S2 
28 
For the dependences shown here, we assume  
that arrays do not overlap in memory (no aliasing).  
Compilers must know that there is no aliasing in order to  
vectorize.  
 
Dependences in Loops (II) 
for (i=1; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i-1] + 2; 
} 
S1 
29 
S2 
Dependences in Loops (II) 
for (i=1; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i-1] + 2; 
} 
S1 
S2 
i=1 
30 
 
 
S1: a[1] = b[1] + 1 
S2: c[1] = a[0] + 2 
 
 
 
S1: a[2] = b[2] + 1 
S2: c[2] = a[1] + 2 
 
 
 
S1: a[3] = b[3] + 1 
S2: c[3] = a[2] + 2 
 
i=2 i=3 
Dependences in Loops (II) 
for (i=1; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i-1] + 2; 
} 
S1 
31 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
1 2 3 4 â€¦ 
â€¦ 
S2 
Dependences in Loops (II) 
for (i=1; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i-1] + 2; 
} 
S1 
32 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
1 2 3 4 â€¦ 
â€¦ 
Loop carried dependence 
S2 
Dependences in Loops (II) 
for (i=1; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i-1] + 2; 
} 
S1 
33 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
1 2 3 4 â€¦ 
â€¦ 
S1 
S2 
For the whole loop 
S2 
Dependences in Loops (II) 
for (i=1; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i-1] + 2; 
} 
S1 
34 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
1 2 3 4 â€¦ 
â€¦ 
S1 
S2 
For the whole loop 
1 
S2 
Dependences in Loops (II) 
â€¢â€¯ Dependences in loops are easy to understand if loops are unrolled. Now the 
dependences are between statement â€œexecutionsâ€ 
for (i=1; i<n; i++){ 
  a[i] = b[i] + 1; 
  c[i] = a[i-1] + 2; 
} 
S1 
35 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
1 2 3 4 â€¦ 
â€¦ 
S1 
S2 
For the whole loop 
1 
distance 
Dependences in Loops (III) 
for (i=0; i<n; i++){ 
  a = b[i] + 1; 
  c[i] = a + 2; 
} 
S1 
S2 
36 
Dependences in Loops (III) 
for (i=0; i<n; i++){ 
  a = b[i] + 1; 
  c[i] = a + 2; 
} 
S1 
S2 
37 
i=0 
 
 
S1: a= b[0] + 1 
S2: c[0] = a + 2 
 
 
 
S1: a = b[1] + 1 
S2: c[1] = a + 2 
 
 
 
S1: a = b[2] + 1 
S2: c[2] = a+ 2 
 
i=1 i=2 
Dependences in Loops (III) 
for (i=0; i<n; i++){ 
  a = b[i] + 1; 
  c[i] = a + 2; 
} 
S1 
S2 
38 
i=0 
 
 
S1: a= b[0] + 1 
S2: c[0] = a + 2 
 
 
 
S1: a = b[1] + 1 
S2: c[1] = a + 2 
 
 
 
S1: a = b[2] + 1 
S2: c[2] = a+ 2 
 
i=1 i=2 
     Loop independent dependence 
     Loop carried dependence 
Dependences in Loops (III) 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
0 1 2 3 â€¦ 
for (i=0; i<n; i++){ 
  a = b[i] + 1; 
  c[i] = a + 2; 
} 
S1 
S2 
39 
Dependences in Loops (III) 
S1 
S2 
iteration: 
instances of S1: 
instances of S2: 
S1 
S2 
S1 
S2 
S1 
S2 
0 1 2 3 â€¦ 
for (i=0; i<n; i++){ 
  a = b[i] + 1; 
  c[i] = a + 2; 
} 
S1 
S2 
40 
S1 
S2 
Loop Vectorization 
â€¢â€¯ Loop Vectorization is not always a legal transformation.  
â€“â€¯ Compilers can vectorize when there are only forward 
dependences 
â€“â€¯ Compilers cannot vectorize when there is a cycle in the data 
dependences (with the exception of self-antidependence), 
unless a transformation is applied to remove the cycle 
â€“â€¯ Codes with only backward dependences can be vectorized, but 
need to be transformed 
41 
forward  
dependence 
S1 
S2 
S1 
S2 
backward  
dependence 
S1 
S2 
cycle 
S1 
S1 
cycle 
Simple Example 
â€¢â€¯ Loop vectorization transforms a program so that the 
same operation is performed at the same time on several 
vector elements 
for (i=0; i<n; i++)  
  c[i] = a[i] + b[i]; 
â€¦ Register File 
X1 
Y1 
Z1 
32 bits 
32 bits 
+ 
32 
bits 
Scalar Unit Vector 
Unit 
lw $t0, 0($a0) 
lw  $t1, 0($a1) 
add $t3, $t0, $t1 
sw $t3, 0($a3) 
  
n  
times 
lwv $vt0, 0($a0) 
lwv $vt1, 0($a1) 
addv $vt3, $vt0, $vt1 
swv %vt3, $0($a3) 
n/4  
times 
42 
Loop Vectorization  
â€¢â€¯ When vectorizing a loop with several statements the 
compiler  need to strip-mine the loop and then apply loop 
distribution  
for (i=0; i<LEN; i++){ 
  a[i]=b[i]+(float)1.0; 
  c[i]=b[i]+(float)2.0; 
} 
S1 
S2 
43 
for (i=0; i<LEN; i+=strip_size){ 
 for (j=i; j<i+strip_size; j++) 
   a[j]=b[j]+(float)1.0; 
   c[j]=b[j]+(float)2.0; 
} 
 
for (i=0; i<LEN; i+=strip_size){ 
 for (j=i; j<i+strip_size; j++) 
   a[j]=b[j]+(float)1.0; 
 for (j=i; j<i+strip_size; j++) 
   c[j]=b[j]+(float)2.0; 
} 
 
Loop Vectorization  
â€¢â€¯ When vectorizing a loop with several statements the 
compiler  needs to strip-mine the loop and then apply 
loop distribution  
for (i=0; i<LEN; i++){ 
  a[i]=b[i]+(float)1.0; 
  c[i]=b[i]+(float)2.0; 
} 
S1 
S2 
44 
for (i=0; i<LEN; i+=strip_size){ 
 for (j=i; j<i+strip_size; j++) 
   a[j]=b[j]+(float)1.0; 
 for (j=i; j<i+strip_size; j++) 
   c[j]=b[j]+(float)2.0; 
} 
 
S2 S2 S2 S2 S2 S2 S2 S2 
S1 S1 S1 S1 
i=0 i=1 i=2 i=3 
S1 S1 S1 S1 
i=4 i=5 i=6 i=7 
Loop Vectorization  
â€¢â€¯ When vectorizing a loop with several statements the 
compiler needs to strip-mine the loop and then apply 
loop distribution  
for (i=0; i<LEN; i++){ 
  a[i]=b[i]+(float)1.0; 
  c[i]=b[i]+(float)2.0; 
} 
S1 
S2 
45 
for (i=0; i<LEN; i+=strip_size){ 
 for (j=i; j<i+strip_size; j++) 
   a[j]=b[j]+(float)1.0; 
 for (j=i; j<i+strip_size; j++) 
   c[j]=b[j]+(float)2.0; 
} 
 
S2 S2 S2 S2 S2 S2 S2 S2 
S1 S1 S1 S1 
i=0 i=1 i=2 i=3 
S1 S1 S1 S1 
i=4 i=5 i=6 i=7 
Acyclic Dependence Graphs: 
Forward Dependences 
	

for (i=0; i<LEN; i++) { 
  a[i]= b[i] + c[i] 
  d[i] = a[i] + (float) 1.0; 
} 
 
 
S1
S2
S1 
S2 
forward  
dependence 
46 
S1 
S2 
S1 
S2 
S1 
S2 
S1 
S2 
i=0 i=1 i=2 i=3 
for (i=0; i<LEN; i++) { 
  a[i]= b[i] + c[i] 
  d[i] = a[i] + (float) 1.0; 
} 
47 
Intel Nehalem 
Compiler report: Loop was 
vectorized 
Exec. Time scalar code: 10.2 
Exec. Time vector code:   6.3 
Speedup: 1.6 
IBM Power 7 
Compiler report: Loop was SIMD 
vectorized 
Exec. Time scalar code: 3.1 
Exec. Time vector code: 1.5 
Speedup: 2.0 
Acyclic Dependence Graphs: 
Forward Dependences 
for (i=0; i<LEN; i++) { 
  a[i]= b[i] + c[i] 
  d[i] = a[i+1] + (float) 1.0; 
} 
 
 
S1 
S2 
48 
S1: a[0] = b[0] + c[0] 
S2: d[0] = a[1] + 1 
S1: a[1] = b[0] + c[0] 
S2: d[1] = a[2] + 1 
 
i=0 
i=1 
Acyclic Dependenden Graphs 
 Backward Dependences (I) 
for (i=0; i<LEN; i++) { 
  a[i]= b[i] + c[i] 
  d[i] = a[i+1] + (float) 1.0; 
} 
 
 S1 
S2 
S1 
S2 backward  
dependence 
49 
S1: a[0] = b[0] + c[0] 
S2: d[0] = a[1] + 1 
S1: a[1] = b[0] + c[0] 
S2: d[1] = a[2] + 1 
 
i=0 
i=1 
S1 
S2 
S1 
S2 
S1 
S2 
S1 
S2 
This loop cannot be vectorized as it is 
Acyclic Dependenden Graphs 
 Backward Dependences (I) 
S1 
S2 
for (i=0; i<LEN; i++) { 
  d[i] = a[i+1]+(float)1.0; 
  a[i]= b[i] + c[i]; 
} 
 
 
S2 
S1 
S1 
S2 
backward  
depedence 
S2 
S1 
forward  
depedence 
50 
Acyclic Dependenden Graphs 
 Backward Dependences (I) 
S1 
S2 
S1 
S2 
S1 
S2 
S2 
S1 
S2 
S1 
S2 
S1 
Reorder of statements 
for (i=0; i<LEN; i++) { 
 a[i]= b[i] + c[i] 
 d[i] = a[i+1] + (float) 1.0; 
} 
 
 
for (i=0; i<LEN; i++) { 
  a[i]= b[i] + c[i]; 
  d[i] = a[i+1]+(float)1.0; 
} 
 
 
for (i=0; i<LEN; i++) { 
  d[i] = a[i+1]+(float)1.0; 
  a[i]= b[i] + c[i]; 
} 
 
 
51 
Intel Nehalem 
Compiler report: Loop was not 
vectorized. Existence of vector 
dependence 
Exec. Time scalar code: 12.6 
Exec. Time vector code: -- 
Speedup: -- 
Intel Nehalem 
Compiler report: Loop was vectorized  
Exec. Time scalar code: 10.7 
Exec. Time vector code: 6.2 
Speedup: 1.72 
Speedup vs non-reordered code:2.03 
Acyclic Dependenden Graphs 
 Backward Dependences (I) 
S1 
S2 
S1 
S2 
for (int i=0;i<LEN-1;i++){     
  a[i]=a[i+1]+b[i]; 
} 
S1 
S1 
for (int i=1;i<LEN;i++){     
  a[i]=a[i-1]+b[i]; 
} 
S1 
Self-antidependence 
can be vectorized 
Self true-dependence 
can not  vectorized  
(as it is) 
a[0]=a[1]+b[0] 
a[1]=a[2]+b[1] 
a[2]=a[3]+b[2] 
a[3]=a[4]+b[3] 
a[1]=a[0]+b[1] 
a[2]=a[1]+b[2] 
a[3]=a[2]+b[3] 
a[4]=a[3]+b[4] 
S1
52 
Cycles in the DG (III) 
for (int i=0;i<LEN-1;i++){     
  a[i]=a[i+1]+b[i]; 
} 
S1 
for (int i=1;i<LEN;i++){     
  a[i]=a[i-1]+b[i]; 
} 
S1
53 
Intel Nehalem 
Compiler report: Loop was 
vectorized 
Exec. Time scalar code: 6.0 
Exec. Time vector code: 2.7 
Speedup: 2.2 
Intel Nehalem 
Compiler report: Loop was not 
vectorized. Existence of vector 
dependence 
Exec. Time scalar code: 7.2 
Exec. Time vector code: -- 
Speedup: -- 
S1 S1 
Cycles in the DG (III) 
Outline 
	

â€¢â€¯ Overcoming limitations to SIMD-Vectorization 
â€“â€¯ Data Dependences 
â€“â€¯ Data Alignment 
â€“â€¯ Aliasing 
â€“â€¯ Non-unit strides 
â€“â€¯ Conditional Statements 
â€¢â€¯ Vectorization with intrinsics  
54 
Data Alignment 
â€¢â€¯ Vector loads/stores load/store 128 consecutive bits to a vector 
register.  
â€¢â€¯ Data addresses need to be 16-byte (128 bits) aligned to be loaded/
stored   
-â€¯ Intel platforms support aligned and unaligned load/stores 
-â€¯ IBM platforms do not support unaligned load/stores 
55 
 
void test1(float *a,float *b,float *c) 
{  
for (int i=0;i<LEN;i++){   
  a[i] = b[i] + c[i]; 
} 
b 
0  1  2  3  
Is &b[0] 16-byte aligned? 
vector load loads b[0] â€¦ b[3] 
 
Data Alignment 
â€¢â€¯ To know if a pointer is 16-byte aligned, the last digit of 
the pointer address in hex must be 0. 
â€¢â€¯ Note that if &b[0] is 16-byte aligned, and is a single 
precision array, then &b[4] is also 16-byte aligned 
56 
Output: 
0x7fff1e9d8580, 0x7fff1e9d8590 
__attribute__ ((aligned(16))) float  B[1024]; 
 
int main(){Â Â  
  printf("%p, %p\n", &B[0], &B[4]); 
} 
Data Alignment 
â€¢â€¯ In many cases, the compiler cannot statically know the 
alignment of the address in a pointer 
â€¢â€¯ The compiler assumes that the base address of the 
pointer is 16-byte aligned and adds a run-time checks for it 
â€“â€¯ if the runtime check is false, then it uses another code 
(which may be scalar) 
57 
Data Alignment 
â€¢â€¯ Manual 16-byte alignment can be achieved by forcing 
the base address to be a multiple of 16.  
58 
__attribute__ ((aligned(16))) float b[N]; 
float* a = (float*) memalign(16,N*sizeof(float)); 
â€¢â€¯ When the pointer is passed to a function, the compiler 
should be aware of where the 16-byte aligned address of 
the array starts.  
void func1(float *a, float *b, 
float *c) { 
  __assume_aligned(a, 16);  
  __assume_aligned(b, 16); 
  __assume_aligned(c, 16); 
for int (i=0; i<LEN; i++) { 
  a[i] = b[i] + c[i]; 
} 
  
Data Alignment - Example 
59 
float A[N] __attribute__((aligned(16)));  
float B[N] __attribute__((aligned(16)));  
float C[N] __attribute__((aligned(16))); 
 
void test(){   
for (int i = 0; i < N; i++){ 
  C[i] = A[i] + B[i]; 
}} 
 
Data Alignment - Example 
60 
float A[N] __attribute__((aligned(16)));  
float B[N] __attribute__((aligned(16)));  
float C[N] __attribute__((aligned(16))); 
 
void test1(){   
__m128 rA, rB, rC;  
 for (int i = 0; i < N; i+=4){     
  rA = _mm_load_ps(&A[i]);     
  rB = _mm_load_ps(&B[i]);     
  rC = _mm_add_ps(rA,rB); 
  _mm_store_ps(&C[i], rC);   
}} 
 
 
void test2(){ 
__m128 rA, rB, rC; 
for (int i = 0; i < N; i+=4){   
  rA = _mm_loadu_ps(&A[i]);     
  rB = _mm_loadu_ps(&B[i]);     
  rC = _mm_add_ps(rA,rB); 
  _mm_storeu_ps(&C[i], rC);   
}} 
void test3(){   
__m128 rA, rB, rC;   
for (int i = 1; i < N-3; i+=4){     
  rA = _mm_loadu_ps(&A[i]); 
  rB = _mm_loadu_ps(&B[i]); 
  rC = _mm_add_ps(rA,rB); 
  _mm_storeu_ps(&C[i], rC);   
}} 
Nanosecond per iteration 
Core 2 Duo Intel i7 Power 7 
Aligned 0.577 0.580 0.156 
Aligned (unaligned ld) 0.689 0.581 0.241 
Unaligned 2.176 0.629 0.243 
Alignment in a struct  
â€¢â€¯ Arrays B and D are not 16-bytes aligned (see the 
address) 
61 
struct st{Â  
  char A;Â Â  
  int B[64];Â  
 Â float C;Â Â  
  int D[64]; 
}; 
 
int main(){Â Â  
  st s1;Â  
  printf("%p, %p, %p, %p\n", &s1.A, s1.B, &s1.C, s1.D);} 
Output: 
0x7fffe6765f00, 0x7fffe6765f04, 0x7fffe6766004, 0x7fffe6766008 
Alignment in a struct  
â€¢â€¯ Arrays A and B are aligned to 16-byes  (notice the 
0 in the 4 least significant bits of the address) 
â€¢â€¯ Compiler automatically does padding 
62 
struct st{Â  
  char A;Â Â  
  int B[64] __attribute__ ((aligned(16)));Â  
 Â float C;Â Â  
  int D[64] __attribute__ ((aligned(16))); 
}; 
 
int main(){Â Â  
  st s1;Â  
  printf("%p, %p, %p, %p\n", &s1.A, s1.B, &s1.C, s1.D);} 
Output: 
0x7fff1e9d8580, 0x7fff1e9d8590, 0x7fff1e9d8690, 0x7fff1e9d86a0 
Aliasing 
â€¢â€¯ Can the compiler vectorize this loop? 
	

63 
void func1(float *a,float *b, float *c){  
   for (int i = 0; i < LEN; i++) {  
       a[i] = b[i] + c[i];  
} 
Aliasing 
â€¢â€¯ Can the compiler vectorize this loop? 
	

64 
void func1(float *a,float *b, float *c) 
{  
   for (int i = 0; i < LEN; i++)  
       a[i] = b[i] + c[i];  
} 
float* a  = &b[1]; 
â€¦ 
b[1]= b[0] + c[0] 
b[2] = b[1] + c[1] 
Aliasing 
â€¢â€¯ Can the compiler vectorize this loop? 
	

65 
void func1(float *a,float *b, float *c) 
{  
   for (int i = 0; i < LEN; i++)   
       a[i] = b[i] + c[i];  
} 
float* a  = &b[1]; 
â€¦ 
a and b are aliasing 
There is a self-true dependence 
Vectorizing this loop would  
be illegal 
void func1(float *a, float *b, float *c){ 
for (int i=0; i<LEN; i++)  
  a[i] = b[i] + c[i]; 
} 
 
â€¢â€¯ To vectorize, the compiler needs to guarantee that the 
pointers are not aliased. 
â€¢â€¯ When the compiler does not know if two pointer are 
alias, it still vectorizes, but needs to add up-to ð‘‚(â€‹ð‘›â†‘2âŸ) 
run-time checks, where n is the number of pointers 
	
When the number of pointers is large, the compiler 
may decide to not vectorize 
Aliasing 
66 
Aliasing 
â€¢â€¯ Two solutions can be used to avoid the run-time 
checks 
1.â€¯ static and global arrays   
2.â€¯__restrict__ attribute  
      	

 
 
 
67 
Aliasing 
1.â€¯ Static and Global arrays 
	

68 
__attribute__ ((aligned(16))) float a[LEN];   
__attribute__ ((aligned(16))) float b[LEN];  
__attribute__ ((aligned(16))) float c[LEN];  
 
void func1(){ 
for (int i=0; i<LEN; i++)  
  a[i] = b[i] + c[i]; 
} 
  
int main() {           
â€¦                
   func1(); 
}  
Aliasing 
1.â€¯__restrict__ keyword 
69 
void func1(float* __restrict__ a,float* b, float* c) { 
  __assume_aligned(a, 16);  
  __assume_aligned(b, 16); 
  __assume_aligned(c, 16); 
  for int (i=0; i<LEN; i++)  
       a[i] = b[i] + c[i]; 
} 
  
int main() {           
   float* a=(float*) memalign(16,LEN*sizeof(float)); 
   float* b=(float*) memalign(16,LEN*sizeof(float)); 
   float* c=(float*) memalign(16,LEN*sizeof(float)); 
   â€¦ 
   func1(a,b,c); 
}  
Non-unit Stride â€“ Example I 
â€¢â€¯ Array of a struct  
70 
typedef struct{int x, y, z} 
point; 
point pt[LEN];  
 
for (int i=0; i<LEN; i++) {  
  pt[i].y *= scale; 
} 
 
point pt[N] x0 y0 z0 x1 y1 z1 x2 y2 z2 x3 y3 z3 
pt[0] pt[1] pt[2] pt[3] 
Non-unit Stride â€“ Example I 
â€¢â€¯ Array of a struct  
71 
typedef struct{int x, y, z} 
point; 
point pt[LEN];  
 
for (int i=0; i<LEN; i++) {  
  pt[i].y *= scale; 
} 
 
point pt[N] x0 y0 z0 x1 y1 z1 x2 y2 z2 
vector load  vector load  
x3 y3 z3 
vector load  
pt[0] pt[1] pt[2] pt[3] 
Non-unit Stride â€“ Example I 
â€¢â€¯ Array of a struct 
72 
typedef struct{int x, y, z} 
point; 
point pt[LEN]; 
 
for (int i=0; i<LEN; i++) {  
  pt[i].y *= scale; 
} 
 
point pt[N] x0 y0 z0 x1 y1 z1 x2 y2 z2 
vector load  
vector register 
(I need) 
y0 y1 y2 
vector load  
y3 
x3 y3 z3 
vector load  
Non-unit Stride â€“ Example I 
â€¢â€¯ Array of a struct 
73 
typedef struct{int x, y, z} 
point; 
point pt[LEN]; 
 
for (int i=0; i<LEN; i++) {  
  pt[i].y *= scale; 
} 
 
point pt[N] x0 y0 z0 x1 y1 z1 x2 y2 z2 
vector load  
vector register 
(I need) 
y0 y1 y2 
vector load  
y3 
int ptx[LEN], int pty[LEN],  
int ptz[LEN]; 
 
for (int i=0; i<LEN; i++) {  
  pty[i] *= scale; 
} 
 
â€¢â€¯ Arrays   
y0 y1 y3 y4 y5 y6 y7 
y0 y1 y2 y3 
y2 
vector load  vector load  
#pragma vector always 
for (int i = 0; i < LEN; i++){ 
  if (c[i] < (float) 0.0) 
    a[i] = a[i] * b[i] + d[i]; 
} 
Conditional Statements â€“ I  
â€¢â€¯ Loops with conditions need #pragma vector always	

â€“â€¯ Since the compiler does not know if vectorization will be 
profitable  
â€“â€¯ The condition may prevent from an exception 
74 
 
for (int i = 0; i < LEN; i++){ 
  if (c[i] < (float) 0.0) 
    a[i] = a[i] * b[i] + d[i]; 
} 
Conditional Statements â€“ I  
75 
Intel Nehalem 
Compiler report: Loop was  not 
vectorized. Condition may protect 
exception 
Exec. Time scalar code: 10.4 
Exec. Time vector code: -- 
Speedup: -- 
Intel Nehalem 
Compiler report: Loop was 
vectorized. 
Exec. Time scalar code: 10.4 
Exec. Time vector code:   5.0 
Speedup: 2.0 
#pragma vector always 
for (int i = 0; i < LEN; i++){ 
  if (c[i] < (float) 0.0) 
    a[i] = a[i] * b[i] + d[i]; 
} 
for (int i = 0; i < LEN; i++){ 
  if (c[i] < (float) 0.0) 
    a[i] = a[i] * b[i] + d[i]; 
} 
Conditional Statements 
â€¢â€¯ Compiler removes if conditions when 
generating vector code 
76 
for (int i=0;i<1024;i++){ 
  if (c[i] < (float) 0.0) 
    a[i]=a[i]*b[i]+d[i]; 
} 
vector bool char = rCmp 
vector float r0={0.,0.,0.,0.}; 
vector float rA,rB,rC,rD,rS, rT, 
rThen,rElse; 
for (int i=0;i<1024;i+=4){ 
  // load rA, rB, and rD;  
  rCmp = vec_cmplt(rC, r0); 
  rT= rA*rB+rD; 
  rThen = vec_and(rT.rCmp); 
  rElse = vec_andc(rA.rCmp); 
  rS = vec_or(rthen, relse); 
  //store rS  
} 
Conditional Statements 
77 
rThen 
rElse 
rC 
rS 
2 -1 1 -2 
rCmp True True False False 
0 3.2 3.2 
1. 0 1. 
3.2 3.2 
0 
0 
1. 1. 
    Speedups will depend on the 
values on c[i] 
 
     Compiler tends to be 
conservative, as the condition 
may prevent from segmentation 
faults 
Conditional Statements 
78 
for (int i=0;i<1024;i++){ 
  if (c[i] < (float) 0.0) 
    a[i]=a[i]*b[i]+d[i]; 
} 
vector bool char = rCmp 
vector float r0={0.,0.,0.,0.}; 
vector float rA,rB,rC,rD,rS, rT, 
rThen,rElse; 
for (int i=0;i<1024;i+=4){ 
  // load rA, rB, and rD;  
  rCmp = vec_cmplt(rC, r0); 
  rT= rA*rB+rD; 
  rThen = vec_and(rT.rCmp); 
  rElse = vec_andc(rA.rCmp); 
  rS = vec_or(rthen, relse); 
  //store rS  
} 
Compiler Directives  
â€¢â€¯ Compiler vectorizes many loops, but many more can be 
vectorized if the appropriate directives are used 
79 
Compiler Hints for Intel ICC Semantics 
#pragma ivdep Ignore assume data dependences 
#pragma vector always  override efficiency heuristics 
#pragma novector disable vectorization  
__restrict__ assert exclusive access through 
pointer 
__attribute__ ((aligned(int-val))) request memory alignment 
memalign(int-val,size); malloc aligned memory 
__assume_aligned(exp, int-val) assert alignment property 
Compiler Directives  
â€¢â€¯ Compiler vectorizes many loops, but many more can be 
vectorized if the appropriate directives are used 
80 
Compiler Hints for IBM XLC Semantics 
#pragma ibm independent_loop Ignore assumed data dependences 
#pragma nosimd disable vectorization  
__restrict__ assert exclusive access through 
pointer 
__attribute__ ((aligned(int-val))) request memory alignment 
memalign(int-val,size); malloc aligned memory 
__alignx (int-val, exp) assert alignment property 
Outline 
	

â€¢â€¯ Overcoming limitations to SIMD-Vectorization 
â€“â€¯ Data Dependences 
â€“â€¯ Data Alignment 
â€“â€¯ Aliasing 
â€“â€¯ Non-unit strides 
â€“â€¯ Conditional Statements 
â€¢â€¯ Vectorization with intrinsics  
81 
Access the SIMD through intrinsics 
â€¢â€¯ Intrinsics are vendor/architecture specific 
â€¢â€¯ We will focus on the Intel vector intrinsics 
â€¢â€¯ Intrinsics are useful when 
â€“â€¯  the compiler fails to vectorize  
â€“â€¯ when the programmer thinks it is possible to generate 
better code than the one produced by the compiler 
82 
The Intel SSE intrinsics Header file 
â€¢â€¯ SSE can be accessed using intrinsics. 
â€¢â€¯ You must use one of the following header files: 
#include <xmmintrin.h> (for SSE) 
#include <emmintrin.h> (for SSE2)  
#include <pmmintrin.h> (for SSE3) 
#include <smmintrin.h> (for SSE4) 
â€¢â€¯ These include the prototypes of the intrinsics. 
83 
Intel SSE intrinsics Data types 
â€¢â€¯ We will use the following data types: 
__m128 packed single precision (vector XMM register) 
__m128d packed double precision (vector XMM register) 
__m128i packed integer (vector XMM register) 
â€¢â€¯ Example 
#include <xmmintrin.h>  
int main ( ) { 
  ... 
  __m128 A, B, C;  /* three packed s.p. variables */ 
  ... 
} 
84 
 
Intel SSE intrinsic Instructions 
 â€¢â€¯ Intrinsics operate on these types and have the format: 
 _mm_instruction_suffix(â€¦) 
â€¢â€¯ Suffix can take many forms. Among them: 
ss scalar single precision 
ps packed (vector) singe precision 
sd scalar double precision 
pd packed double precision 
si# scalar integer (8, 16, 32, 64, 128 bits) 
su# scalar unsigned integer (8, 16, 32, 64, 128 bits) 
85 
Intel SSE intrinsics 
Instructions â€“ Examples 
â€¢â€¯ Load four 16-byte aligned single precision values in a 
vector: 
    float a[4]={1.0,2.0,3.0,4.0};//a must be 16-byte aligned 
    __m128 x = _mm_load_ps(a); 
 
 
â€¢â€¯ Add two vectors containing four single precision values: 
    __m128 a, b; 
  __m128 c = _mm_add_ps(a, b); 
86 
 
Intrinsics (SSE) 
 
#include <xmmintrin.h> 
#define n 1024 
__attribute__((aligned(16))) float 
a[n], b[n], c[n]; 
 
int main() { 
__m128 rA, rB, rC; 
for (i = 0; i < n; i+=4) { 
  rA = _mm_load_ps(&a[i]); 
  rB = _mm_load_ps(&b[i]); 
  rC= _mm_mul_ps(rA,rB); 
  _mm_store_ps(&c[i], rC); 
}} 
87 
#define n 1024 
__attribute__ ((aligned(16))) 
float a[n], b[n], c[n]; 
 
 
int main() { 
for (i = 0; i < n; i++) { 
  c[i]=a[i]*b[i]; 
 } 
} 
Intel SSE intrinsics 
A complete example 
88 
Header file #define n 1024 
 
 
int main() { 
float a[n], b[n], c[n]; 
for (i = 0; i < n; i+=4) { 
  c[i:i+3]=a[i:i+3]+b[i:i+3]; 
 } 
} 
#include <xmmintrin.h> 
#define n 1024 
__attribute__((aligned(16))) float 
a[n], b[n], c[n]; 
 
int main() { 
__m128 rA, rB, rC; 
for (i = 0; i < n; i+=4) { 
  rA = _mm_load_ps(&a[i]); 
  rB = _mm_load_ps(&b[i]); 
  rC= _mm_mul_ps(rA,rB); 
  _mm_store_ps(&c[i], rC); 
}} 
Intel SSE intrinsics 
A complete example 
89 
#define n 1024 
 
 
int main() { 
float a[n], b[n], c[n]; 
for (i = 0; i < n; i+=4) { 
  c[i:i+3]=a[i:i+3]+b[i:i+3]; 
 } 
} 
Declare 3 vector 
registers 
#include <xmmintrin.h> 
#define n 1024 
__attribute__((aligned(16))) float 
a[n], b[n], c[n]; 
 
int main() { 
__m128 rA, rB, rC; 
for (i = 0; i < n; i+=4) { 
  rA = _mm_load_ps(&a[i]); 
  rB = _mm_load_ps(&b[i]); 
  rC= _mm_mul_ps(rA,rB); 
  _mm_store_ps(&c[i], rC); 
}} 
Intel SSE intrinsics 
A complete example 
90 
Execute vector 
statements 
#define n 1000 
 
 
int main() { 
float a[n], b[n], c[n]; 
for (i = 0; i < n; i+=4) { 
  c[i:i+3]=a[i:i+3]+b[i:i+3]; 
 } 
} 
#include <xmmintrin.h> 
#define n 1024 
__attribute__((aligned(16))) float 
a[n], b[n], c[n]; 
 
int main() { 
__m128 rA, rB, rC; 
for (i = 0; i < n; i+=4) { 
  rA = _mm_load_ps(&a[i]); 
  rB = _mm_load_ps(&b[i]); 
  rC= _mm_mul_ps(rA,rB); 
  _mm_store_ps(&c[i], rC); 
}} 
Node Splitting  
for (int i=0;i<LEN-1;i++){  
  a[i]=b[i]+c[i]; 
  d[i]=(a[i]+a[i+1])*(float)0.5;  
} 
 
S1 
S2 
for (int i=0;i<LEN-1;i++){  
  temp[i]=a[i+1]; 
  a[i]=b[i]+c[i]; 
  d[i]=(a[i]+temp[i])*(float) 0.5;  
} 
 
S1 
S2 
S1 
S2 
S0 
S0 S1 
S2 
91 
Node Splitting with intrinsics 
for (int i=0;i<LEN-1;i++){  
  a[i]=b[i]+c[i]; 
  d[i]=(a[i]+a[i+1])*(float)0.5;  
} 
 
for (int i=0;i<LEN-1;i++){  
  temp[i]=a[i+1]; 
  a[i]=b[i]+c[i]; 
  d[i]=(a[i]+temp[i])*(float)0.5;  
} 
 
92 
#include <xmmintrin.h> 
#define n 1000 
 
int main() { 
__m128 rA1, rA2, rB, rC, rD; 
__m128 r5=_mm_set1_ps((float)0.5) 
for (i = 0; i < LEN-4; i+=4) { 
  rA2= _mm_loadu_ps(&a[i+1]); 
  rB= _mm_load_ps(&b[i]); 
  rC= _mm_load_ps(&c[i]); 
  rA1= _mm_add_ps(rB, rC); 
  rD= _mm_mul_ps(_mm_add_ps(rA1,rA2),r5); 
  _mm_store_ps(&a[i], rA1); 
  _mm_store_ps(&d[i], rD); 
}} 
Which code runs faster ? 
Why? 
Node Splitting with intrinsics 
for (int i=0;i<LEN-1;i++){  
  a[i]=b[i]+c[i]; 
  d[i]=(a[i]+a[i+1])*(float)0.5;  
} 
 
for (int i=0;i<LEN-1;i++){  
  temp[i]=a[i+1]; 
  a[i]=b[i]+c[i]; 
  d[i]=(a[i]+temp[i])*(float)0.5;  
} 
 
93 
#include <xmmintrin.h> 
#define n 1000 
 
int main() { 
__m128 rA1, rA2, rB, rC, rD; 
__m128 r5=_mm_set1_ps((float)0.5) 
for (i = 0; i < LEN-4; i+=4) { 
  rA2= _mm_loadu_ps(&a[i+1]); 
  rB= _mm_load_ps(&b[i]); 
  rC= _mm_load_ps(&c[i]); 
  rA1= _mm_add_ps(rB, rC); 
  rD= _mm_mul_ps(_mm_add_ps(rA1,rA2),r5); 
  _mm_store_ps(&a[i], rA1); 
  _mm_store_ps(&d[i], rD); 
}} 
Node Splitting with intrinsics 
94 
Intel Nehalem 
Compiler report: Loop was not 
vectorized. Existence of vector 
dependence 
Exec. Time scalar code: 12.6 
Exec. Time vector code: -- 
Speedup: -- 
Intel Nehalem 
Compiler report: Loop was 
vectorized.  
Exec. Time scalar code: 13.2 
Exec. Time vector code: 9.7 
Speedup: 1.3 
Intel Nehalem 
Exec. Time intrinsics: 6.1 
Speedup (versus vector code): 1.6 
Summary 
â€¢â€¯ Microprocessor vector extensions can contribute to improve program performance 
and the amount of this contribution is likely to increase in the future as vector 
lengths grow. 
â€¢â€¯ Compilers are only partially successful at vectorizing  
â€¢â€¯ When the compiler fails, programmers can 
â€“â€¯ add compiler directives  
â€“â€¯ apply loop transformations 
â€¢â€¯ If after transforming the code, the compiler still fails to vectorize (or the 
performance of the generated code is poor), the only option is to program the 
vector extensions directly using intrinsics or assembly language. 
95 
 
Data Dependences 
	

â€¢â€¯ The correctness of many many loop transformations including 
vectorization can be decided using dependences.  
â€¢â€¯ A good introduction to the notion of dependence and its applications 
can be found in D. Kuck, R. Kuhn, D. Padua, B. Leasure, M. Wolfe: 
Dependence Graphs and Compiler Optimizations. POPL 1981. 
96 
Compiler Optimizations 
â€¢â€¯  For a longer discussion see:  
â€“â€¯ Kennedy, K. and Allen, J. R. 2002 Optimizing Compilers for Modern 
Architectures: a Dependence-Based Approach. Morgan Kaufmann Publishers 
Inc. 
â€“â€¯ U. Banerjee. Dependence Analysis for Supercomputing. Kluwer Academic 
Publishers, Norwell, Mass., 1988. 
â€“â€¯ Advanced Compiler Optimizations for Supercomputers, by David Padua and 
Michael Wolfe in Communications of the ACM, December 1986, Volume 29, 
Number 12.  
â€“â€¯ Compiler Transformations for High-Performance Computing, by David Bacon, 
Susan Graham and Oliver Sharp, in ACM Computing Surveys, Vol. 26, No. 4, 
December 1994.  
 
97 
Algorithms 
â€¢â€¯ W. Daniel Hillis and Guy L. Steele, Jr.. 1986. 
Data parallel algorithms. Commun. ACM 29, 12 
(December 1986), 1170-1183. 
â€¢â€¯ Shyh-Ching Chen, D.J. Kuck, "Time and Parallel 
Processor Bounds for Linear Recurrence 
Systems," IEEE Transactions on Computers, pp. 
701-717, July, 1975  
 
98 
Measuring execution time 
99 
time1 = time(); 
 for (i=0; i<32000; i++)  
  c[i] = a[i] + b[i]; 
  
time2 = time(); 
Measuring execution time 
â€¢â€¯ Added an outer loop that runs (serially) 
â€“â€¯ to increase the running time of the loop 
100 
time1 = time(); 
for (j=0; j<200000; j++){
 for (i=0; i<32000; i++)  
  c[i] = a[i] + b[i]; 
  
} 
time2 = time(); 
Measuring execution times 
â€¢â€¯ Added an outer loop that runs (serially) 
â€“â€¯ to increase the running time of the loop 
â€¢â€¯ Call a dummy () function that is compiled separately  
â€¢â€¯ to avoid loop interchange or dead code elimination 
101 
time1 = time(); 
for (j=0; j<200000; j++){  
 for (i=0; i<32000; i++)  
  c[i] = a[i] + b[i]; 
 dummy(); 
} 
time2 = time(); 
Measuring execution times 
â€¢â€¯ Added an outer loop that runs (serially) 
â€“â€¯ to increase the running time of the loop 
â€¢â€¯ Call a dummy () function that is compiled separately  
â€¢â€¯ to avoid loop interchange or dead code elimination 
â€¢â€¯ Access the elements of one output array and print the result  
â€“â€¯ to avoid dead code elimination  
102 
time1 = time(); 
for (j=0; j<200000; j++){  
 for (i=0; i<32000; i++)  
  c[i] = a[i] + b[i]; 
 dummy(); 
} 
time2 = time(); 
for (j=0; j<32000; j++) 
   ret+= a[i]; 
printf (â€œTime %f, result %fâ€, (time2 â€“time1), ret); 
Compiling 
â€¢â€¯ Intel icc scalar code  
 icc -O3 â€“no-vec dummy.o tsc.o â€“o runnovec 
â€¢â€¯ Intel icc vector code  
 icc -O3 â€“vec-report[n] â€“xSSE4.2 dummy.o tsc.o â€“o runvec 
  
 [n] can be 0,1,2,3,4,5  
 â€“ vec-report0, no report is generated 
 â€“ vec-report1, indicates the line number of the loops that were 
vectorized 
 â€“ vec-report2 .. 5, gives a more detailed report that includes the loops 
that were not vectorized and the reason for that.  
 
 
103 
Compiling 
flags = -O3 â€“qaltivec -qhot -qarch=pwr7 -qtune=pwr7  
    -qipa=malloc16 -qdebug=NSIMDCOST  
    -qdebug=alwaysspec â€“qreport 
 
â€¢â€¯ IBM xlc scalar code  
 xlc -qnoenablevmx dummy.o tsc.o â€“o runnovec 
â€¢â€¯ IBM vector code  
 xlc â€“qenablevmx dummy.o tsc.o â€“o runvec 
  
 
104 
105 
106 
Compiler Directives (I) 
â€¢â€¯ When the compiler does not vectorize automatically due 
to dependences the programmer can inform the compiler 
that it is safe to vectorize: 
#pragma ivdep (ICC compiler)  
 
#pragma ibm independent_loop (XLC compiler) 
	

	

	

107 
Compiler Directives (I) 
â€¢â€¯ This loop can be vectorized when k < -3 and k >= 0. 
â€¢â€¯ Programmer knows that k>=0 
	

	

108 
Â for (int i=val;i<LEN-k;i++)Â Â Â Â  
     a[i]=a[i+k]+b[i]; 
 
a[0]=a[1]+b[0] 
a[1]=a[2]+b[1] 
a[2]=a[3]+b[2] 
a[1]=a[0]+b[0] 
a[2]=a[1]+b[1] 
a[3]=a[2]+b[2] 
k =-1 
If (k >= 0) ïƒ  no dependence  
or self-anti-dependence 
If (k <0) ïƒ  self-true dependence 
Can  
be vectorized k =1 
Cannot   
be vectorized 
S1 
S1 
Compiler Directives (I) 
â€¢â€¯ This loop can be vectorized when k < -3 and k >= 0. 
â€¢â€¯ Programmer knows that k>=0 
	

	

109 
Â for (int i=val;i<LEN-k;i++)Â Â Â Â  
     a[i]=a[i+k]+b[i]; 
 
How can the programmer tell the 
compiler that k >= 0	

Compiler Directives (I) 
â€¢â€¯ This loop can be vectorized when k < -3 and k >= 0. 
â€¢â€¯ Programmer knows that k>=0 
	

	

110 
#pragma ivdepÂ   
wrong results will be 	

obtained if loop is vectorized	

when  -3 < k < 0	

Intel ICC provides the #pragma ivdep to 	

tell the compiler that it is safe to ignore	

unknown dependences	

Â for (int i=val;i<LEN-k;i++)Â Â Â Â  
     a[i]=a[i+k]+b[i]; 
 
Compiler Directives (I) 
111 
Â   for (int i=0;i<LEN-k;i++)Â Â Â Â  
     a[i]=a[i+k]+b[i]; 
S124 S124_1 
Intel Nehalem 
Compiler report: Loop was not 
vectorized. Existence of vector 
dependence 
Exec. Time scalar code: 6.0 
Exec. Time vector code: -- 
Speedup: -- 
Intel Nehalem 
Compiler report: Loop was 
vectorized 
Exec. Time scalar code: 6.0 
Exec. Time vector code: 2.4 
Speedup: 2.5 
S124 and S124_1 S124_2 
if (k>=0) 
 #pragma ivdep 
 for (int i=0;i<LEN-k;i++)Â Â Â Â  
     a[i]=a[i+k]+b[i]; 
if (k<0) 
  for (int i=0);i<LEN-k;i++) 
     a[i]=a[i+k]+b[i]; 
if (k>=0) 
 for (int i=0;i<LEN-k;i++)Â  
     a[i]=a[i+k]+b[i]; 
if (k<0) 
  for (int i=0);i<LEN-k;i++) 
     a[i]=a[i+k]+b[i]; 
          
S124_2 
Compiler Directives (I) 
112 
IBM Power 7 
Compiler report: Loop was not 
vectoriced because a data  
dependence prevents SIMD 
vectorization 
Exec. Time scalar code: 2.2 
Exec. Time vector code: --  
Speedup: -- 
 #pragma ibm independent_loop 
needs AIX OS (we ran the 
experiments on Linux) 
Â   for (int i=0;i<LEN-k;i++)Â Â Â Â  
     a[i]=a[i+k]+b[i]; 
S124 S124_1 
if (k>=0) 
 for (int i=0;i<LEN-k;i++)Â Â Â Â  
     a[i]=a[i+k]+b[i]; 
if (k<0) 
  for (int i=0);i<LEN-k;i++) 
     a[i]=a[i+k]+b[i]; 
          
S124_2 
S124 and S124_1 S124_2 
if (k>=0) 
 #pragma ibm independent_loop 
 for (int i=0;i<LEN-k;i++)Â Â Â Â  
     a[i]=a[i+k]+b[i]; 
if (k<0) 
  for (int i=0);i<LEN-k;i++) 
     a[i]=a[i+k]+b[i]; 
Strip Mining 
 
	
This transformation improves locality and is usually 
combined with vectorization	

113 
Strip Mining 
- first statement can be vectorized 
- second statement cannot be  
   vectorized because of self-true  
   dependence 
  
114 
for (i=1; i<LEN; i++) { 
  a[i]= b[i]; 
  c[i] = c[i-1] + a[i]; 
} 
 
 
for (i=1; i<LEN; i++)  
  a[i]= b[i]; 
 
for (i=1; i<LEN; i++)  
  c[i] = c[i-1] + a[i]; 
 
 By applying loop distribution the  
compiler will vectorize the first  
statement 
But, â€¦ loop distribution will increase  
the cache miss ratio if array a[] is large 
S1 
S2 
Strip Mining 
 	

115 
for (i=1; i<LEN; i
+=strip_size){  
  for (j=i; j<strip_size; j++)  
    a[j]= b[j]; 
  for (j=i; j<strip_size; j++)  
    c[j] = c[j-1] + a[j]; 
} 
 
    
for (i=1; i<LEN; i++)  
  a[i]= b[i]; 
for (i=1; i<LEN; i++)  
  c[i] = c[i-1] + a[i]; 
 
 
Loop Distribution  Strip Mining 
strip_size is usually a small value (4, 8, 16 or 32). 
Strip Mining 
â€¢â€¯ Another example 
116 
int v[N]; 
â€¦ 
for (int i=0;i<N;i++){ 
  Transform (v[i]); 
} 
for (int i=0;i<N;i++){ 
  Light (v[i]); 
} 
 
int v[N]; 
â€¦ 
for (int i=0;i<N;i+=strip_size){ 
  for (int j=i;j<strip_size;j++){ 
    Transform (v[j]);  
   } 
  for (int j=i;i<strip_size;j++){ 
    Light (v[j]); 
   } 
} 
Compiler Directives (I) 
â€¢â€¯ When the compiler does not vectorize automatically due 
to dependences the programmer can inform the compiler 
that it is safe to vectorize: 
#pragma ivdep (ICC compiler)  
 
#pragma ibm independent_loop (XLC compiler) 
	

	

	

117 
Compiler Directives (II) 
â€¢â€¯ Programmer can disable vectorization of a loop when the 
when the vector code runs slower than the scalar code 
	

	

	

118 
#pragma novector (ICC compiler)  
 
#pragma nosimd (XLC compiler) 
	

S1 
S2 
for (int i=1;i<LEN;i++){  
  a[i] = b[i] + c[i]; 
  d[i] = a[i] + e[i-1]; 
  e[i] = d[i] + c[i]; 
} 
 
S1 
S2 
Vector code can run slower than scalar code   
S3 
S3 
S1 can be vectorized 
S2 and S3 cannot be vectorized (as they are)   
119 
Compiler Directives (II) 
Less locality when  
executing in vector mode 
#pragma novector 
120 
S116 
Intel Nehalem 
Compiler report: Loop was 
partially vectorized 
Exec. Time scalar code: 14.7 
Exec. Time vector code: 18.1 
Speedup: 0.8 
S116 
Compiler Directives (II) 
for (int i=1;i<LEN;i++){  
  a[i] = b[i] + c[i]; 
  d[i] = a[i] + e[i-1]; 
  e[i] = d[i] + c[i]; 
} 
 
